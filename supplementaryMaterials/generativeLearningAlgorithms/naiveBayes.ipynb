{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;In GDA, the feature vectors $x$ were continuous, real-valued vectors. Lets now talk about a different learning algorithm in which the $x$'s are discrete-valued.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Multi-variate Bernoulli Event Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;For our motivating example, consider building an email spam filter using machine learning. Here, we wish to classify messages according to whether they are unsolicited commercial (spam) email, or non-spam email. After learning to do this, we can then have our mail reader automatically filter out the spam messages and perhaps place them in a separate mail folder. Classifying emails is one example of a broader set of problems called **text classification**.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;**We will represent an email via a feature vector whose length is equal to the number of words in the dictionary.** Specifically, if an email contains the *$i$*-th word of the dictionary, then we will set $x_i=1$; otherwise, we let $x_i=0$. For instance, the vector\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "$$\n",
    "x = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 1 \\\\ \\vdots \\\\ 0 \\end{bmatrix}, \\quad\n",
    "\\begin{matrix} a \\\\ aardvark \\\\ aardwolf \\\\ \\vdots \\\\ buy \\\\ \\vdots \\\\ zygmurgy \\end{matrix}\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;is used to represent an email that contains the words \"a\" and \"buy\", but not \"aardvark\", \"aardwolf\" or \"zygmurgy\". The set of words encoded into the feature is called the **vocabulary**, so the dimension of $x$ is equal to the size of the vocabulary.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;Actually, rather than looking through an english dictionary for the list of all english words, in practice it is more common to look through our training set and encode in our feature vector only the words that occur at least once there. Apart from reducing the number of words modeled and hence reducing our computational and space requirements, this also has the advantage of allowing us to model/include as a feature many words that may appear in your email (such as \"cs229\") but that you won't find in dictionary. Sometimes (as in the homework), we also exclude the very high frequency words (which will be words like \"the\", \"of\", \"and\"; these high frequency, \"content free\" words are called **stop words**) since they occur in so many documents and do little to indicate whether an email is spam or non-spam.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;Having chosen our feature vector, we now want to build a discriminative model. So, we have to model $p(x\\,|\\,y)$. But if we have, say, a vocabulary of 50000 words, then $x\\in\\{0,\\,1\\}^{50000}$ ( $x$ is a 50000-dimensional vector of 0's and 1's ), and if we were to model $x$ explicitly with a multinomial distribution over the $2^{50000}$ possible outcomes, then we'd end up with a ($2^{50000}-1$)-dimensional parameter vector. This is clearly too many parameters.\n",
    "</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;To model $p(x\\,|\\,y)$, we will therefore make a very strong assumption. We will assume that the $x_i$'s are conditionally independent given $y$. This assumption is called the **Naive Bayes (NB) assumption**, and the resulting algorithm is called the **Naive Bayes classifier**.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;We now have:\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "$$\n",
    "\\begin{align}\n",
    "p(x_1,\\cdots,x_{50000}\\,|\\,y) \n",
    "&= p(x_1\\,|\\,y)p(x_2\\,|\\,y,x_1)p(x_3\\,|\\,y,x_1,x_2){\\cdots}p(x_{50000}\\,|\\,y,x_1,\\cdots,x_{49999}) \\\\\n",
    "&= p(x_1\\,|\\,y)p(x_2\\,|\\,y)p(x_3\\,|\\,y){\\cdots}p(x_{50000}\\,|\\,y) \\\\\n",
    "&= \\prod_{i=1}^{n}p(x_i\\,|\\,y)\n",
    "\\end{align}\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;The first equality simply follows from the usual properties of probabilities, and the second equality used the NB assumption. We note that even though the Naive Bayes assumption is an extremely strong assumptions, the resulting algorithm works well on many problems.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;Our model is parameterized by\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "$$\n",
    "\\begin{align}\n",
    "\\phi_{i\\,|\\,y=1}&=p(x_i=1\\,|\\,y=1) \\\\\n",
    "\\phi_{i\\,|\\,y=0}&=p(x_i=1\\,|\\,y=0) \\\\\n",
    "\\phi_y&=p(y=1)\n",
    "\\end{align}\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;As usual, given a training set $\\{(x^{(i)}, y^{(i)});i=1,\\cdots,m\\}$, we can write down the joint likelihood of the data:\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "$$\n",
    "\\mathcal{L}(\\phi_y,\\phi_{i\\,|\\,y=0},\\phi_{i\\,|\\,y=1}) = \\prod_{i=1}^mp(x^{(i)},y^{(i)})\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;Maximizing this with respect to $\\phi_y,\\phi_{i\\,|\\,y=0},\\phi_{i\\,|\\,y=1}$ gives the maximum likelihood estimates:\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "\\begin{align}\n",
    "\\phi_{j\\,|\\,y=1} &= \\frac{\\sum_{i=1}^{m}1\\{x_j^{(i)}=1\\,\\land\\,y^{(i)}=1\\}}{\\sum_{i=1}^{m}1\\{y^{(i)}=1\\}} \\\\\n",
    "\\phi_{j\\,|\\,y=0} &= \\frac{\\sum_{i=1}^{m}1\\{x_j^{(i)}=1\\,\\land\\,y^{(i)}=0\\}}{\\sum_{i=1}^{m}1\\{y^{(i)}=0\\}} \\\\\n",
    "\\phi_{y} &= \\frac{\\sum_{i=1}^m1\\{y^{(i)}=1\\}}{m}\n",
    "\\end{align}\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;In the equations above, the \"$\\land$\" symbol means \"and\". The parameters have a very natural interpretation. For instance, $\\phi_{j\\,|\\,y=1}$ is just the fraction of the spam ($y=1$) emails in which word $j$ does appear.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;Having fit all these parameters, to make a prediction on a new example with features $x$, we then simply calculate:\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "$$\n",
    "\\begin{align}\n",
    "p(y=1\\,|\\,x) \n",
    "&= \\frac{p(x\\,|\\,y=1)p(y=1)}{p(x)} \\\\\n",
    "&= \\frac{(\\prod^n_{i=1}p(x_i\\,|\\,y=1))\\,p(y=1)}{(\\prod^n_{i=1}p(x_i\\,|\\,y=1))\\,p(y=1)+(\\prod^n_{i=1}p(x_i\\,|\\,y=0))\\,p(y=0)}\n",
    "\\end{align}\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "and pick whichever class has the higher posterior probability.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 详细推导："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;已知\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "$$\n",
    "\\begin{align}\n",
    "y\\,&\\sim\\,Bernoulli(\\phi_y) \\\\\n",
    "x_j\\,|\\,y=0\\,&\\sim\\,Bernoulli(\\phi_{j\\,|\\,y=0}) \\\\\n",
    "x_j\\,|\\,y=1\\,&\\sim\\,Bernoulli(\\phi_{j\\,|\\,y=1})\n",
    "\\end{align}\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;对数似然函数\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "$$\n",
    "\\begin{align}\n",
    "l(\\phi_y,\\phi_{j\\,|\\,y=0},\\phi_{j\\,|\\,y=1})\n",
    "&= log\\prod_{i=1}^mp(x^{(i)},y^{(i)}) \\\\\n",
    "&= log\\prod_{i=1}^mp(x^{(i)}\\,|\\,y^{(i)})p(y^{(i)}) \\\\\n",
    "&= \\sum_{i=1}^{m}logp(x^{(i)}\\,|\\,y^{(i)})+\\sum_{i=1}^mlog\\,p(y^{(i)}) \\\\\n",
    "&= \\sum_{i=1}^mlog\\prod^{n}_{j=1}p(x^{(i)}_j\\,|\\,y^{(i)}) + \\sum_{i=1}^mlog\\,p(y^{(i)}) \\\\\n",
    "&= \\sum_{i=1}^mlog\\prod^{n}_{j=1}p(x^{(i)}_j\\,|\\,y^{(i)}=0)^{1-y^{(i)}}\\,{\\cdot}\\,p(x^{(i)}_j\\,|\\,y^{(i)}=1)^{y^{(i)}} + \\sum_{i=1}^mlog\\,p(y^{(i)}) \\\\\n",
    "&= \\sum_{i=1}^m\\sum^n_{j=1}\\left( (1-y^{(i)})\\,log\\,p(x^{(i)}_j\\,|\\,y^{(i)}=0)+y^{(i)}\\,log\\,p(x^{(i)}_j\\,|\\,y^{(i)}=1) \\right)+ \\sum_{i=1}^mlog\\,p(y^{(i)}) \\\\\n",
    "&= \\sum_{i=1}^m\\sum^n_{j=1}(1-y^{(i)})\\,log\\,p(x^{(i)}_j\\,|\\,y^{(i)}=0)+\\sum_{i=1}^m\\sum^n_{j=1}y^{(i)}\\,log\\,p(x^{(i)}_j\\,|\\,y^{(i)}=1) + \\sum_{i=1}^mlog\\,p(y^{(i)})\n",
    "\\end{align}\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;注意，此函数分为三个部分，其中，$\\phi_{j\\,|\\,y=0}$ 只与第一部分有关，$\\phi_{j\\,|\\,y=1}$ 只与第二部分有关，$\\phi_y$ 只与第三部分有关。\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "首先，求 $\\phi_{j\\,|\\,y=0}$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "\\begin{align}\n",
    "\\triangledown_{\\phi_{j\\,|\\,y=0}}l(\\phi_y,\\phi_{j\\,|\\,y=0},\\phi_{j\\,|\\,y=1})\n",
    "&= \\triangledown_{\\phi_{j\\,|\\,y=0}}\\sum^m_{i=1}\\sum^n_{j=1}(1-y^{(i)})\\,log\\,p(x^{(i)}_j\\,|\\,y^{(i)}=0) \\\\\n",
    "&= \\triangledown_{\\phi_{j\\,|\\,y=0}}\\sum^m_{i=1}\\sum^n_{j=1}(1-y^{(i)})\\,log\\,\\phi_{j\\,|\\,y=0}^{x_j^{(i)}}\\,\\cdot\\,(1-\\phi_{j\\,|\\,y=0})^{(1-x_j^{(i)})} \\\\\n",
    "&= \\triangledown_{\\phi_{j\\,|\\,y=0}}\\sum^m_{i=1}\\sum^n_{j=1}1\\{y^{(i)}=0\\}\\left( x_j^{(i)}\\,log\\,\\phi_{j\\,|\\,y=0}+(1-x_j^{(i)})\\,log\\,(1-\\phi_{j\\,|\\,y=0}) \\right) \\\\\n",
    "&= \\sum^m_{i=1}1\\{y^{(i)}=0\\}\\left( x_j^{(i)}\\frac{1}{\\phi_{j\\,|\\,y=0}}-(1-x_j^{(i)})\\frac{1}{1-\\phi_{j\\,|\\,y=0}} \\right) \\\\\n",
    "&= \\frac{\\sum^m_{i=1}1\\{y^{(i)}=0\\}x_j^{(i)}}{\\phi_{j\\,|\\,y=0}}-\\frac{\\sum^m_{i=1}1\\{y^{(i)}=0\\}(1-x_j^{(i)})}{1-\\phi_{j\\,|\\,y=0}}\n",
    "\\end{align}\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;令其为零，即\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\sum^m_{i=1}1\\{y^{(i)}=0\\}x_j^{(i)}}{\\phi_{j\\,|\\,y=0}} &= \\frac{\\sum^m_{i=1}1\\{y^{(i)}=0\\}(1-x_j^{(i)})}{1-\\phi_{j\\,|\\,y=0}} \\\\\n",
    "\\sum^m_{i=1}1\\{y^{(i)}=0\\}x_j^{(i)}-\\phi_{j\\,|\\,y=0}\\sum^m_{i=1}1\\{y^{(i)}=0\\}x_j^{(i)} &= \\phi_{j\\,|\\,y=0}\\sum^m_{i=1}1\\{y^{(i)}=0\\}(1-x_j^{(i)}) \\\\\n",
    "\\sum^m_{i=1}1\\{y^{(i)}=0\\}1\\{x_j^{(i)}=1\\} &= \\phi_{j\\,|\\,y=0}\\sum^m_{i=1}1\\{y^{(i)}=0\\} \\\\\n",
    "\\phi_{j\\,|\\,y=0} &= \\frac{\\sum^m_{i=1}1\\{x_j^{(i)}=1\\,{\\land}\\,y^{(i)}=0\\}}{\\sum^m_{i=1}1\\{y^{(i)}=0\\}}\n",
    "\\end{align}\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;$\\phi_{j\\,|\\,y=1}$，同理可得。\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "最后，求 $\\phi_y$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "$$\n",
    "\\begin{align}\n",
    "\\triangledown{\\phi_y}l(\\phi_y,\\phi_{j\\,|\\,y=0},\\phi_{j\\,|\\,y=1})\n",
    "&= \\triangledown{\\phi_y}\\sum^{m}_{i=1}log\\,p(y^{(i)}) \\\\\n",
    "&= \\triangledown{\\phi_y}\\sum^{m}_{i=1}log\\,\\phi_y^{y^{(i)}}(1-\\phi_y)^{(1-y^{(i)})} \\\\\n",
    "&= \\triangledown{\\phi_y}\\sum^{m}_{i=1}\\left( y^{(i)}log\\phi_y+(1-y^{(i)})log(1-\\phi_y) \\right) \\\\\n",
    "&= \\sum^{m}_{i=1}y^{(i)}\\frac{1}{\\phi_y}-\\sum^{m}_{i=1}(1-y^{(i)})\\frac{1}{1-\\phi_y}\n",
    "\\end{align}\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;令其为零，可得\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "$$\n",
    "\\begin{align}\n",
    "\\sum^{m}_{i=1}y^{(i)}\\frac{1}{\\phi_y} &= \\sum^{m}_{i=1}(1-y^{(i)})\\frac{1}{1-\\phi_y} \\\\\n",
    "\\sum^m_{i=1}y^{(i)}-\\phi_y\\sum^m_{i=1}y^{(i)} &= \\phi_y\\sum^m_{i=1}(1-y^{(i)}) \\\\\n",
    "\\sum^m_{i=1}1\\{y^{(i)}=1\\}-\\phi_y\\sum^m_{i=1}1\\{y^{(i)}=1\\} &= \\phi_y\\sum^m_{i=1}1\\{y^{(i)}=0\\} \\\\\n",
    "\\sum^m_{i=1}1\\{y^{(i)}=1\\} &= m\\phi_y \\\\\n",
    "\\phi_y &= \\frac{\\sum^m_{i=1}1\\{y^{(i)}=1\\}}{m}\n",
    "\\end{align}\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;Lastly, we note that while we have developed the Naive Bayes algorithm mainly for the case of problems where the features $x_i$ are binary-valued, the generalization to where $x_i$ can take values in $\\{1,2,\\cdots,k\\}$ is straightforward. Here, we would simply model $p(x_i\\,|\\,y)$ as multinomial rather than as Bernoulli. Indeed, even if some original inpute attribute (say, the living area of a house, as in our earlier example) were continuous valued, it is quite common to **discretize** it——that is turn it into a small set of discrete values——and apply Naive Bayes. \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;When the original, continuous-valued attributes are well-modeled by a multivariate normal distribution, discretizing the features and using Naive Bayes (instead of GDA) will often result in a better classifier.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2. Laplace Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;The Naive Bayes alogorithm as we have described it will work fairly well for many problems, but there is a simple change that makes it work much better, especially for text classification. Lets briefly discuss a problem with the algorithm in its current form, and then talk about how we can fix it.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;Consider spam/email classification, and lets suppose that, after completing CS229 and having done excellent work on the project, you decide around June 2003 to submit the work you did to the NIPS conference for publication.(NIPS is one of the top machine learning conferences, and the deadline for submitting a paper is typically in late June or early July.) Because you end up discussing the conference in your emails, you also start getting messages with the word \"nips\" in it. But this is your first NIPS paper, and until this time, you had not previously seem any emails containing the word \"nips\"; in particular \"nips\" did not ever appear in your training set of spam/non-spam emails. Assuming that \"nips\" was the 35000th word in the dictionary, your Naive Bayes spam filter therefore had picked its maximum likelihood estimates of the parameters $\\phi_{35000\\,|\\,y}$ to be\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "$$\n",
    "\\begin{align}\n",
    "\\phi_{35000\\,|\\,y=1} &= \\frac{\\sum^{m}_{i=1}1\\{x_{35000}^{(i)}=1\\,\\land\\,y^{(i)}=1\\}}{\\sum^{m}_{i=1}1\\{y^{(i)}=1\\}}=0 \\\\\n",
    "\\phi_{35000\\,|\\,y=0} &= \\frac{\\sum^{m}_{i=1}1\\{x_{35000}^{(i)}=1\\,\\land\\,y^{(i)}=0\\}}{\\sum^{m}_{i=1}1\\{y^{(i)}=0\\}}=0\n",
    "\\end{align}\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;I.e., because it has never seen \"nips\" before in either spam or non-spam training examples, it thinks the probability of seeing it in either type of email is zero. Hence, when trying to decide if one of these messages containing \"nips\" is spam, it calculates the class posterior probailities, and obtains\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "$$\n",
    "\\begin{align}\n",
    "p(y=1\\,|\\,x) &= \\frac{\\prod^n_{i=1}p(x_i\\,|\\,y=1)p(y=1)}{\\prod^n_{i=1}p(x_i\\,|\\,y=1)p(y=1)+\\prod^n_{i=1}p(x_i|y=0)p(y=0)} \\\\\n",
    "&= \\frac{0}{0}\n",
    "\\end{align}\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;This is because each of the terms \"$\\prod^n_{i=1}p(x_i\\,|\\,y)$\" includes a term $p(x_{35000}\\,|\\,y)=0$ that is multiplied into it. Hence, our algorithm obtains $0/0$, and doesn't know how to make a prediction.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;Stating the problem more broadly, it is statistically a bad idea to estimate the probability of some event to be zero just because you haven't seen it before in your finite training set. Take the problem of estimating the mean of a multinomial random variable $z$ taking values in $\\{1,\\cdots,k\\}$. We can parameterize our multinomial with $\\phi_i=p(z=i)$. Given a set of $m$ independent observations $\\{z^{(1)},\\cdots,z^{(m)}\\}$, the maximum likelihood estimates are given by\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "$$\n",
    "\\phi_j=\\frac{\\sum^m_{i=1}1\\{z^{(i)}=j\\}}{m}\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;As we saw previously, if we were to use these maximum likelihood estimates, then some of the $\\phi_j$'s might end up as zero, which was a problem. To avoid this, we can use **Laplace smoothing**, which replaces the above estimate with\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "$$\n",
    "\\phi_j=\\frac{\\sum^m_{i=1}1\\{z^{(i)}=j\\}+1}{m+k}\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;Here we've added 1 to the numerator, and $k$ to the denominator. Note that $\\sum^k_{j=1}\\phi_j=1$ still holds, which is a desirable property since the $\\phi_j$'s are estimates for probabilities that we know must sum to 1. Also, $\\phi_j\\not=0$ for all values of $j$, sovling our problem of probabilities being estimated as zero. Under certain (arguably quite strong) conditions, it can be shown that the Laplace smoothing actually gives the optimal estimator of the $\\phi_j$'s.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;Returning to our Naive Bayes classifier, with Laplace smoothing, we therefore obtain the following estimates of the parameters:\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "$$\n",
    "\\begin{align}\n",
    "\\phi_{j\\,|\\,y=1} \n",
    "&= \\frac{\\sum^m_{i=1}1\\{x_j^{(i)}=1\\,\\land\\,y^{(i)}=1\\}+1}{\\sum^{m}_{i=1}1\\{y^{(i)}=1\\}+2} \\\\\n",
    "\\phi_{j\\,|\\,y=0} \n",
    "&= \\frac{\\sum^m_{i=1}1\\{x_j^{(i)}=1\\,\\land\\,y^{(i)}=0\\}+1}{\\sum^{m}_{i=1}1\\{y^{(i)}=0\\}+2}\n",
    "\\end{align}\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;In practice, it usually doesn't matter much whether we apply Laplace smoothing to $\\phi_y$ or not, since we will typically have a fair fraction each of spam and non-spam messages, so $\\phi_y$ will be a reasonable estimate of $p(y=1)$ and will be quite far from 0 anyway.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3. Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;建立一个朴素贝叶斯分类器，来判断某个句子是否是侮辱性句子。\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = [['my','dog','has','flea','problems','help','please'],\n",
    "        ['maybe','not','take','him','to','dog','park','stupid'],\n",
    "        ['my','dalmation','is','so','cute','I','love','him'],\n",
    "        ['stop','posting','stupid','worthless','garbage'],\n",
    "        ['mr','licks','ate','my','steak','how','to','stop','him'],\n",
    "        ['quit','buying','worthless','dog','food','stupid']]\n",
    "\n",
    "labels = [0, 1, 0, 1, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 建立词汇表\n",
    "def createVocab(sentences):\n",
    "    \n",
    "    vocab = set()\n",
    "    for sentence in sentences:\n",
    "        # 取并集\n",
    "        vocab = vocab | set(sentence)\n",
    "    \n",
    "    return list(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 对输入的句子进行向量化\n",
    "def sentence2Vec(vocab, sentence):\n",
    "    \n",
    "    vec = np.zeros(len(vocab))\n",
    "    for word in sentence:\n",
    "        if word in vocab:\n",
    "            vec[vocab.index(word)] = 1\n",
    "        else:\n",
    "            print(\"{}不在词汇表中\".format(word))\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 准备数据\n",
    "def loadData(sentences, labels, vocab):\n",
    "    \n",
    "    # 准备 X_train，y_train，其中 X_train 是一个 (m, len(vocab)) 的矩阵\n",
    "    # m 为训练数据的个数，即句子的个数\n",
    "    X_train = np.zeros((len(sentences), len(vocab)))\n",
    "    for index, sentence in enumerate(sentences):\n",
    "        X_train[index] = sentence2Vec(vocab, sentence)\n",
    "    \n",
    "    y_train = np.array(labels)\n",
    "    \n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainNB(X_train, y_train):\n",
    "    \n",
    "    # m 为训练数据的个数\n",
    "    m = X_train.shape[0]\n",
    "    \n",
    "    num_pos = y_train.sum() # 标签为1的样本数\n",
    "    num_neg = m - num_pos   # 标签为0的样本数\n",
    "    \n",
    "    phi_pos = (X_train[y_train == 1, :].sum(axis=0) + 1) / (num_pos + 2) # 记录每个词在正样本出现的概率\n",
    "    phi_neg = (X_train[y_train == 0, :].sum(axis=0) + 1) / (num_neg + 2) # 记录每个词在负样本出现的概率\n",
    "    \n",
    "    phi = num_pos / m # 记录phi_y\n",
    "    \n",
    "    return phi_pos, phi_neg, phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;在利用训练得到的参数进行预测的时候，会出现下溢出的问题。这是由于太多很小的数相乘导致的。例如，在计算 $\\prod^{n}_{i=1}p(x_i\\,|\\,y=1)$ 的时候，由于大部分因子都非常小，所以程序会出现下溢出或者得到不正确的答案。其中一种解决办法是对乘积取自然对数。\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;因此，可得\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "$$\n",
    "\\begin{align}\n",
    "log\\,p(y=1\\,|\\,x)\n",
    "&= log\\frac{p(x\\,|\\,y=1)p(y=1)}{p(x)} \\\\\n",
    "&= log\\,p(x\\,|\\,y=1)+log\\,p(y=1)-log\\,p(x)\n",
    "\\end{align}\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;下溢出的问题出现在 $log\\,p(x\\,|\\,y=1)$，所以下面只关注 $log\\,p(x\\,|\\,y=1)$ 的计算，即\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "\\begin{align}\n",
    "log\\,p(x\\,|\\,y=1)\n",
    "&= log\\prod^n_{i=1}p(x_i\\,|\\,y=1) \\\\\n",
    "&= \\sum^n_{i=1}log\\,p(x_i\\,|\\,y=1) \\\\\n",
    "&= \\sum^n_{i=1}log\\,\\phi_{i\\,|\\,y=1}^{x_i}\\,\\cdot\\,(1-\\phi_{i\\,|\\,y=1})^{(1-x_i)} \\\\\n",
    "&= \\sum^n_{i=1}\\left( x_ilog\\,\\phi_{i\\,|\\,y=1}+(1-x_i)log\\,(1-\\phi_{i\\,|\\,y=1}) \\right)\n",
    "\\end{align}\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predictNB(matrix, state):\n",
    "    \n",
    "    predictions = np.zeros(matrix.shape[0])\n",
    "   \n",
    "    phi_pos, phi_neg, phi = state\n",
    "\n",
    "    pos = matrix * phi_pos + (1 - matrix) * (1 - phi_pos)\n",
    "    p_pos = np.sum(np.log(pos), axis=1) + np.log(phi)\n",
    "\n",
    "    neg = matrix * phi_neg + (1 - matrix) * (1 - phi_neg) \n",
    "    p_neg = np.sum(np.log(neg), axis=1) + np.log(1-phi)\n",
    "    \n",
    "    # p(y=1|x) 与 p(y=0|x) 的分母是一样的，因此只比较分子\n",
    "    predictions[p_pos > p_neg] = 1\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 1., 0., 1.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = createVocab(sentences)\n",
    "X_train, y_train = loadData(sentences, labels, vocab)\n",
    "state = trainNB(X_train, y_train)\n",
    "predictions = predictNB(X_train, state)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试的例子\n",
    "sentences = [['love', 'my', 'dalmation'],\n",
    "            ['stupid', 'garbage']]\n",
    "labels = [0, 1]\n",
    "\n",
    "X_train, y_train = loadData(sentences, labels, vocab)\n",
    "predictions = predictNB(X_train, state)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
