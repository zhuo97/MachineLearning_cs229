{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;In GDA, the feature vectors $x$ were continuous, real-valued vectors. Lets now talk about a different learning algorithm in which the $x$'s are discrete-valued.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Multi-variate Bernoulli Event Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;For our motivating example, consider building an email spam filter using machine learning. Here, we wish to classify messages according to whether they are unsolicited commercial (spam) email, or non-spam email. After learning to do this, we can then have our mail reader automatically filter out the spam messages and perhaps place them in a separate mail folder. Classifying emails is one example of a broader set of problems called **text classification**.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;We will **represent an email** via a feature vector whose length is equal to the number of words in the dictionary. Specifically, if an email contains the *$i$*-th word of the dictionary, then we will set $x_i=1$; otherwise, we let $x_i=0$. For instance, the vector\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "$$\n",
    "x = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 1 \\\\ \\vdots \\\\ 0 \\end{bmatrix}, \\quad\n",
    "\\begin{matrix} a \\\\ aardvark \\\\ aardwolf \\\\ \\vdots \\\\ buy \\\\ \\vdots \\\\ zygmurgy \\end{matrix}\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;is used to represent an email that contains the words \"a\" and \"buy\", but not \"aardvark\", \"aardwolf\" or \"zygmurgy\". The set of words encoded into the feature is called the **vocabulary**, so the dimension of $x$ is equal to the size of the vocabulary.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;Actually, rather than looking through an english dictionary for the list of all english words, in practice it is more common to look through our training set and encode in our feature vector only the words that occur at least once there. Apart from reducing the number of words modeled and hence reducing our computational and space requirements, this also has the advantage of allowing us to model/include as a feature many words that may appear in your email (such as \"cs229\") but that you won't find in dictionary. Sometimes (as in the homework), we also exclude the very high frequency words (which will be words like \"the\", \"of\", \"and\"; these high frequency, \"content free\" words are called **stop words**) since they occur in so many documents and do little to indicate whether an email is spam or non-spam.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;Having chosen our feature vector, we now want to build a discriminative model. So, we have to model $p(x\\,|\\,y)$. But if we have, say, a vocabulary of 50000 words, then $x\\in\\{0,\\,1\\}^{50000}$ ( $x$ is a 50000-dimensional vector of 0's and 1's ), and if we were to model $x$ explicitly with a multinomial distribution over the $2^{50000}$ possible outcomes, then we'd end up with a ($2^{50000}-1$)-dimensional parameter vector. This is clearly too many parameters.\n",
    "</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;To model $p(x\\,|\\,y)$, we will therefore make a very strong assumption. We will assume that the $x_i$'s are conditionally independent given $y$. This assumption is called the **Naive Bayes (NB) assumption**, and the resulting algorithm is called the **Naive Bayes classifier**.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;We now have:\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "$$\n",
    "\\begin{align}\n",
    "p(x_1,\\cdots,x_{50000}\\,|\\,y) \n",
    "&= p(x_1\\,|\\,y)p(x_2\\,|\\,y,x_1)p(x_3\\,|\\,y,x_1,x_2){\\cdots}p(x_{50000}\\,|\\,y,x_1,\\cdots,x_{49999}) \\\\\n",
    "&= p(x_1\\,|\\,y)p(x_2\\,|\\,y)p(x_3\\,|\\,y){\\cdots}p(x_{50000}\\,|\\,y) \\\\\n",
    "&= \\prod_{i=1}^{n}p(x_i\\,|\\,y)\n",
    "\\end{align}\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;The first equality simply follows from the usual properties of probabilities, and the second equality used the NB assumption. We note that even though the Naive Bayes assumption is an extremely strong assumptions, the resulting algorithm works well on many problems.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;Our model is parameterized by\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "$$\n",
    "\\begin{align}\n",
    "\\phi_{i\\,|\\,y=1}&=p(x_i=1\\,|\\,y=1) \\\\\n",
    "\\phi_{i\\,|\\,y=0}&=p(x_i=1\\,|\\,y=0) \\\\\n",
    "\\phi_y&=p(y=1)\n",
    "\\end{align}\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;As usual, given a training set $\\{(x^{(i)}, y^{(i)});i=1,\\cdots,m\\}$, we can write down the joint likelihood of the data:\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "$$\n",
    "\\mathcal{L}(\\phi_y,\\phi_{i\\,|\\,y=0},\\phi_{i\\,|\\,y=1}) = \\prod_{i=1}^mp(x^{(i)},y^{(i)})\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;Maximizing this with respect to $\\phi_y,\\phi_{i\\,|\\,y=0},\\phi_{i\\,|\\,y=1}$ gives the maximum likelihood estimates:\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "\\begin{align}\n",
    "\\phi_{j\\,|\\,y=1} &= \\frac{\\sum_{i=1}^{m}1\\{x_j^{(i)}=1\\,\\land\\,y^{(i)}=1\\}}{\\sum_{i=1}^{m}1\\{y^{(i)}=1\\}} \\\\\n",
    "\\phi_{j\\,|\\,y=0} &= \\frac{\\sum_{i=1}^{m}1\\{x_j^{(i)}=1\\,\\land\\,y^{(i)}=0\\}}{\\sum_{i=1}^{m}1\\{y^{(i)}=0\\}} \\\\\n",
    "\\phi_{y} &= \\frac{\\sum_{i=1}^m1\\{y^{(i)}=1\\}}{m}\n",
    "\\end{align}\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;In the equations above, the \"$\\land$\" symbol means \"and\". The parameters have a very natural interpretation. For instance, $\\phi_{j\\,|\\,y=1}$ is just the fraction of the spam ($y=1$) emails in which word $j$ does appear.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;Having fit all these parameters, to make a prediction on a new example with features $x$, we then simply calculate:\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "$$\n",
    "\\begin{align}\n",
    "p(y=1\\,|\\,x) \n",
    "&= \\frac{p(x\\,|\\,y=1)p(y=1)}{p(x)} \\\\\n",
    "&= \\frac{(\\prod^n_{i=1}p(x_i\\,|\\,y=1))\\,p(y=1)}{(\\prod^n_{i=1}p(x_i\\,|\\,y=1))\\,p(y=1)+(\\prod^n_{i=1}p(x_i\\,|\\,y=0))\\,p(y=0)}\n",
    "\\end{align}\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "and pick whichever class has the higher posterior probability.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 详细推导："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;已知\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "$$\n",
    "\\begin{align}\n",
    "y\\,&\\sim\\,Bernoulli(\\phi_y) \\\\\n",
    "x_j\\,|\\,y=0\\,&\\sim\\,Bernoulli(\\phi_{j\\,|\\,y=0}) \\\\\n",
    "x_j\\,|\\,y=1\\,&\\sim\\,Bernoulli(\\phi_{j\\,|\\,y=1})\n",
    "\\end{align}\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;对数似然函数\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "$$\n",
    "\\begin{align}\n",
    "l(\\phi_y,\\phi_{j\\,|\\,y=0},\\phi_{j\\,|\\,y=1})\n",
    "&= log\\prod_{i=1}^mp(x^{(i)},y^{(i)}) \\\\\n",
    "&= log\\prod_{i=1}^mp(x^{(i)}\\,|\\,y^{(i)})p(y^{(i)}) \\\\\n",
    "&= \\sum_{i=1}^{m}logp(x^{(i)}\\,|\\,y^{(i)})+\\sum_{i=1}^mlog\\,p(y^{(i)}) \\\\\n",
    "&= \\sum_{i=1}^mlog\\prod^{n}_{j=1}p(x^{(i)}_j\\,|\\,y^{(i)}) + \\sum_{i=1}^mlog\\,p(y^{(i)}) \\\\\n",
    "&= \\sum_{i=1}^mlog\\prod^{n}_{j=1}p(x^{(i)}_j\\,|\\,y^{(i)}=0)^{1-y^{(i)}}\\,{\\cdot}\\,p(x^{(i)}_j\\,|\\,y^{(i)}=1)^{y^{(i)}} + \\sum_{i=1}^mlog\\,p(y^{(i)}) \\\\\n",
    "&= \\sum_{i=1}^m\\sum^n_{j=1}\\left( (1-y^{(i)})\\,log\\,p(x^{(i)}_j\\,|\\,y^{(i)}=0)+y^{(i)}\\,log\\,p(x^{(i)}_j\\,|\\,y^{(i)}=1) \\right)+ \\sum_{i=1}^mlog\\,p(y^{(i)}) \\\\\n",
    "&= \\sum_{i=1}^m\\sum^n_{j=1}(1-y^{(i)})\\,log\\,p(x^{(i)}_j\\,|\\,y^{(i)}=0)+\\sum_{i=1}^m\\sum^n_{j=1}y^{(i)}\\,log\\,p(x^{(i)}_j\\,|\\,y^{(i)}=1) + \\sum_{i=1}^mlog\\,p(y^{(i)})\n",
    "\\end{align}\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;注意，此函数分为三个部分，其中，$\\phi_{j\\,|\\,y=0}$ 只与第一部分有关，$\\phi_{j\\,|\\,y=1}$ 只与第二部分有关，$\\phi_y$ 只与第三部分有关。\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "首先，求 $\\phi_{j\\,|\\,y=0}$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "\\begin{align}\n",
    "\\triangledown_{\\phi_{j\\,|\\,y=0}}l(\\phi_y,\\phi_{j\\,|\\,y=0},\\phi_{j\\,|\\,y=1})\n",
    "&= \\triangledown_{\\phi_{j\\,|\\,y=0}}\\sum^m_{i=1}\\sum^n_{j=1}(1-y^{(i)})\\,log\\,p(x^{(i)}_j\\,|\\,y^{(i)}=0) \\\\\n",
    "&= \\triangledown_{\\phi_{j\\,|\\,y=0}}\\sum^m_{i=1}\\sum^n_{j=1}(1-y^{(i)})\\,log\\,\\phi_{j\\,|\\,y=0}^{x_j^{(i)}}\\,\\cdot\\,(1-\\phi_{j\\,|\\,y=0})^{(1-x_j^{(i)})} \\\\\n",
    "&= \\triangledown_{\\phi_{j\\,|\\,y=0}}\\sum^m_{i=1}\\sum^n_{j=1}1\\{y^{(i)}=0\\}\\left( x_j^{(i)}\\,log\\,\\phi_{j\\,|\\,y=0}+(1-x_j^{(i)})\\,log\\,(1-\\phi_{j\\,|\\,y=0}) \\right) \\\\\n",
    "&= \\sum^m_{i=1}1\\{y^{(i)}=0\\}\\left( x_j^{(i)}\\frac{1}{\\phi_{j\\,|\\,y=0}}-(1-x_j^{(i)})\\frac{1}{1-\\phi_{j\\,|\\,y=0}} \\right) \\\\\n",
    "&= \\frac{\\sum^m_{i=1}1\\{y^{(i)}=0\\}x_j^{(i)}}{\\phi_{j\\,|\\,y=0}}-\\frac{\\sum^m_{i=1}1\\{y^{(i)}=0\\}(1-x_j^{(i)})}{1-\\phi_{j\\,|\\,y=0}}\n",
    "\\end{align}\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;令其为零，即\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\sum^m_{i=1}1\\{y^{(i)}=0\\}x_j^{(i)}}{\\phi_{j\\,|\\,y=0}} &= \\frac{\\sum^m_{i=1}1\\{y^{(i)}=0\\}(1-x_j^{(i)})}{1-\\phi_{j\\,|\\,y=0}} \\\\\n",
    "\\sum^m_{i=1}1\\{y^{(i)}=0\\}x_j^{(i)}-\\phi_{j\\,|\\,y=0}\\sum^m_{i=1}1\\{y^{(i)}=0\\}x_j^{(i)} &= \\phi_{j\\,|\\,y=0}\\sum^m_{i=1}1\\{y^{(i)}=0\\}(1-x_j^{(i)}) \\\\\n",
    "\\sum^m_{i=1}1\\{y^{(i)}=0\\}1\\{x_j^{(i)}=1\\} &= \\phi_{j\\,|\\,y=0}\\sum^m_{i=1}1\\{y^{(i)}=0\\} \\\\\n",
    "\\phi_{j\\,|\\,y=0} &= \\frac{\\sum^m_{i=1}1\\{x_j^{(i)}=1\\,{\\land}\\,y^{(i)}=0\\}}{\\sum^m_{i=1}1\\{y^{(i)}=0\\}}\n",
    "\\end{align}\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;$\\phi_{j\\,|\\,y=1}$，同理可得。\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "最后，求 $\\phi_y$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "$$\n",
    "\\begin{align}\n",
    "\\triangledown{\\phi_y}l(\\phi_y,\\phi_{j\\,|\\,y=0},\\phi_{j\\,|\\,y=1})\n",
    "&= \\triangledown{\\phi_y}\\sum^{m}_{i=1}log\\,p(y^{(i)}) \\\\\n",
    "&= \\triangledown{\\phi_y}\\sum^{m}_{i=1}log\\,\\phi_y^{y^{(i)}}(1-\\phi_y)^{(1-y^{(i)})} \\\\\n",
    "&= \\triangledown{\\phi_y}\\sum^{m}_{i=1}\\left( y^{(i)}log\\phi_y+(1-y^{(i)})log(1-\\phi_y) \\right) \\\\\n",
    "&= \\sum^{m}_{i=1}y^{(i)}\\frac{1}{\\phi_y}-\\sum^{m}_{i=1}(1-y^{(i)})\\frac{1}{1-\\phi_y}\n",
    "\\end{align}\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;令其为零，可得\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "$$\n",
    "\\begin{align}\n",
    "\\sum^{m}_{i=1}y^{(i)}\\frac{1}{\\phi_y} &= \\sum^{m}_{i=1}(1-y^{(i)})\\frac{1}{1-\\phi_y} \\\\\n",
    "\\sum^m_{i=1}y^{(i)}-\\phi_y\\sum^m_{i=1}y^{(i)} &= \\phi_y\\sum^m_{i=1}(1-y^{(i)}) \\\\\n",
    "\\sum^m_{i=1}1\\{y^{(i)}=1\\}-\\phi_y\\sum^m_{i=1}1\\{y^{(i)}=1\\} &= \\phi_y\\sum^m_{i=1}1\\{y^{(i)}=0\\} \\\\\n",
    "\\sum^m_{i=1}1\\{y^{(i)}=1\\} &= m\\phi_y \\\\\n",
    "\\phi_y &= \\frac{\\sum^m_{i=1}1\\{y^{(i)}=1\\}}{m}\n",
    "\\end{align}\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;Lastly, we note that while we have developed the Naive Bayes algorithm mainly for the case of problems where the features $x_i$ are binary-valued, the generalization to where $x_i$ can take values in $\\{1,2,\\cdots,k\\}$ is straightforward. Here, we would simply model $p(x_i\\,|\\,y)$ as multinomial rather than as Bernoulli. Indeed, even if some original inpute attribute (say, the living area of a house, as in our earlier example) were continuous valued, it is quite common to **discretize** it——that is turn it into a small set of discrete values——and apply Naive Bayes. \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;When the original, continuous-valued attributes are well-modeled by a multivariate normal distribution, discretizing the features and using Naive Bayes (instead of GDA) will often result in a better classifier.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;\n",
    "</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
