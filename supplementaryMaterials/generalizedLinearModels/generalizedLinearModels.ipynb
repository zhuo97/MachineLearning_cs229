{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalized Linear Models\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;In the regression example, we had $y|x;\\theta\\sim\\mathcal{N}(\\mu,\\sigma^2)$, and in the classification one, $y|x;\\theta\\sim{Bernoulli}(\\phi)$, where for some appropriate definitions of $\\mu$ and $\\phi$ as functions of $x$ and $\\theta$.  \n",
    "&emsp;&emsp;In this section, we will show that both of these methods are special cases of a broader family of models, called **Generalized Linear Models (GLMs)**. We will also show how other models in the GLM family can be derived and applied to other classification and regression problems.\n",
    "</font>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The exponential family"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;To work our way up to GLMs, we will begin by defining exponential family distributions. We say that a class of distributions is in the exponential family if it can be written in the form\n",
    "</font>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "$$p(y;\\eta)=b(y)\\,exp(\\,\\eta^TT(y)-a(\\eta)\\,) \\quad - \\quad (1)$$\n",
    "</font>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;Here, $\\eta$ is called the **natural parameter** (also called the **canonical parameter**) of the distribution; T(y) is the **sufficient statistic** (for the distributions we consider, it will often be the case that T(y) = y); and $a(\\eta)$ is the **log partition function**. The quantity $e^{-a(\\eta)}$ essentially plays the role of a normalization constant, that makes sure that the distribution $p(y;\\eta)$ sums/integrates over y to 1.\n",
    "</font>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;A fixed choice of T, a and b defines a *family* (or set) of distributions that is parameterized by $\\eta$; as we vary $\\eta$, we then get different distributions within this family.\n",
    "</font>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;We now show that the Bernoulli and Gaussian distributions are examples of exponentional family distributions.\n",
    "</font>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Bernoulli distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;The Bernoulli distribution with mean $\\phi$, written Bernoulli($\\phi$), specifies a distribution over $y\\in\\{0,1\\}$, so that $p(y=1;\\phi)=\\phi$; $p(y=0;\\phi)=1-\\phi$. As we varying $\\phi$, we obtain Bernoulli distributions with different means. We now show that this class of Bernoulli distributions, ones obtained by varying $\\phi$, is in exponential family; i.e., that there is a choice of T, a and b so that Equation (1) becomes exactly the class of Bernoulli distributions.\n",
    "</font>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;We write the Bernoulli distribution as:\n",
    "</font>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "$$\n",
    "\\begin{align}\n",
    "p(y;\\phi) &= \\phi^y(1-\\phi)^{1-y} \\\\\n",
    "&= exp(\\,ylog\\phi+(1-y)log(1-\\phi)\\,) \\\\\n",
    "&= exp(\\,ylog\\phi-ylog(1-\\phi)+log(1-\\phi)\\,) \\\\\n",
    "&= exp(\\,(log(\\frac{\\phi}{1-\\phi}))y+log(1-\\phi)\\,)\n",
    "\\end{align}\n",
    "$$\n",
    "</font>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;Thus, the natural parameter is given by $\\eta=log(\\frac{\\phi}{1-\\phi})$\n",
    "</font>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;Interestingly, if we invert this definition for $\\eta$ by solving for $\\phi$ in terms of $\\eta$, we obtain $\\phi=\\frac{1}{1+e^{-\\eta}}$. This is the familiar sigmoid function! This will come up again when we derive logistic regression as a GLM. To compelete the formulation of the Bernoulli distribution as an exponential family distribution, we also have\n",
    "</font>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "$$\n",
    "\\begin{align}\n",
    "T(y) &= y \\\\\n",
    "a(\\eta) &= -log(1-\\phi) \\\\\n",
    "&= -log(1-\\frac{1}{1+e^{-\\eta}}) \\\\\n",
    "&= log(1+e^{\\eta}) \\\\\n",
    "b(y) &= 1\n",
    "\\end{align}\n",
    "$$\n",
    "</font>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;This shows that the Bernoulli distribution can be written in the form of Equation (1), using an appropriate choice of T, a and b.\n",
    "</font>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Gaussian distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;Lets now move on to consider the Gaussian distribution. Recall that, when deriving liner regression, the value of $\\sigma^2$ had no effect on our final choice of $\\theta$ and $h_\\theta(x)$. Thus, we can choose an arbitrary value for $\\sigma^2$ without changing anything. To simplify the derivation below, let's set $\\sigma^2=1$. We then have:\n",
    "</font>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "$$\n",
    "\\begin{align}\n",
    "p(y;\\mu) &= \\frac{1}{\\sqrt{2\\pi}}exp(\\,-\\frac{1}{2}(y-\\mu)^2\\,) \\\\\n",
    "&= \\frac{1}{\\sqrt{2\\pi}}exp(\\,-\\frac{1}{2}y^2\\,)\\cdot{exp(\\,\\mu{y}-\\frac{1}{2}\\mu^2\\,)}\n",
    "\\end{align}\n",
    "$$\n",
    "</font>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;Thus, we see that Gaussian is in the exponential family, with\n",
    "</font>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "$$\n",
    "\\begin{align}\n",
    "\\eta &= \\mu \\\\\n",
    "T(y) &= y \\\\\n",
    "a(\\eta) &= \\frac{1}{2}\\mu^2 \\\\\n",
    "&= \\frac{1}{2}\\eta^2 \\\\ \n",
    "b(y) &= \\frac{1}{\\sqrt{2\\pi}}exp(\\,-\\frac{1}{2}y^2\\,) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "</font>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<font size=4>\n",
    "&emsp;&emsp;There're many other distributions that are members of the exponential family: The multinomial (which we'll see later), the Poisson (for modelling count-data); the gamma and exponential (for modelling continuous, non-negative random variables, such as time-intervals); the beta and the Dirichlet (for distributions over probabilities); and many more. In the next section, we will describe a general \"recipe\" for constructing models in which $y$ (given $x$ and $\\theta$) comes from any of these distributions.\n",
    "</font>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Constructing GLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 小结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;在普通线性回归中，我们假设被解释变量 $y$ 是连续的且服从正态分布，同时，被解释变量 $y$ 的期望与解释变量 $x$ 之间的关系是线性关系。  \n",
    "&emsp;&emsp;然而在实际中，被解释变量 $y$ 有可能是离散的，而且有可能不服从正态分布。因此，广义线性模型对普通线性回归进行了推广，放宽了普通线性回归的假设。首先，被解释变量 $y$ 的分布属于某一指数分布族。有很多我们所熟悉的分布都属于指数分布族，包括正态分布、泊松分布、二项分布等等。其次，被解释变量 $y$ 的期望的函数（即 $\\eta$ ）与解释变量 $x$ 之间的关系为线性关系。  \n",
    "&emsp;&emsp;在指数分布族中，未知参数为 $\\eta$， 而我们想要求得的参数为权重向量 $\\theta$。 $h_\\theta(x)$ 的作用正是将二者关联起来，因此，$h_\\theta(x)$ 也被称为连接函数。在广义线性模型中，参数 $\\eta$ 其实是属于指数分布族的概率分布的某一参数（如正态分布的参数 $\\mu$ ，伯努利分布的参数 $\\phi$ 等）的函数，例如，$\\eta=f(\\phi)$，而连接函数则是其反函数，即 $\\phi=f^{-1}(\\eta)$。解出反函数后，将 $\\eta=\\theta^Tx$ 带入其中，即可得到 $h_\\theta(x)$。  \n",
    "&emsp;&emsp;总之，广义线性模型本质上还是一个线性模型。我们推广的只是被解释变量 $y$ 的分布。\n",
    "</font>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2.2 Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;Suppose you would like to build a model to estimate the number $y$ of customers arriving in your store (or number of page-views on your website) in any given hour, based on certain features $x$ such as store promotions, recent advertising, weather, day-of-week, etc. We know that the Poisson distribution usually gives a good model for numbers of visitors. Knowing this, how can we come up with a model for our problem? Fortunately, the Poisson is an exponential family distribution, so we can apply a Gerneralized Liner Model (GLM). In this section, we will describe a method for constructing GLM models for problems such as these.\n",
    "</font>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;More generally, consider a classification or regression problem where we would like to predict the value of some random variable $y$ as a function of $x$. To derive a GLM for this problem, we will make the following three assumptions about the conditional distribution of $y$ given $x$ and about our model:\n",
    "</font>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. <font size=4>$y\\,|\\,x;\\theta\\sim{ExponentialFamily(\\eta)}$. I.e., given $x$ and $\\theta$, the distribution of $y$ follows some exponential family distribution, with parameter $\\eta$.</font>\n",
    "2. <font size=4>Given $x$, our goal is to predict the expected value of $T(y)$. In most of our examples, we will have $T(y)=y$, so this means we would like the prediction $h(x)$ output by our learned hypothesis $h$ to satisfy $h(x)=E[y|x]$. (Note that this assumption is satisfied in the choices for $h_\\theta(x)$ for both logistic regression and linear regression, in logistic regression, we had $h_\\theta(x)=p(y=1|x;\\theta)=0)=0\\cdot{p(y=0|x;\\theta)}+1\\cdot{p(y=1|x;\\theta)}=E[y|x;\\theta]$.)</font>\n",
    "3. <font size=4>The natural parameter $\\eta$ and the inputs $x$ are related linearly: $\\eta=\\theta^Tx$. (Or, if $\\eta$ is vector-valued, then $\\eta_i=\\theta^T_ix$)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;The third of these assumptions might seem the least well justified of the above, and it might be better thought of as a \"design choice\" in our recipe for desiging GLMs, rather than as an assumption per se. These three assumptions/design choices will allow us to derive a very elegant class of learning algorithms, namely GLMs, that have many desirable properties such as ease of learning.\n",
    "</font>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Ordinary Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;To show that ordinary least squares is a special case of the GLM family of models, consider the setting where the target variale $y$ (also called the **response variable** in GLM terminology) is continuous, and we model the conditional distribution of $y$ given $x$ as a Gaussian $\\mathcal{N}(\\mu,\\sigma^2)$. (Here, $\\mu$ may depend $x$.) So, we let the $ExponentialFamily(\\eta)$ distribution above be the Gaussian distribution. As we saw previously, in the formulation of the Gaussian as an exponential family distribution, we had $\\mu=\\eta$. So, we have \n",
    "</font>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "$$\n",
    "\\begin{align}\n",
    "h_\\theta(x) &= E[y|x;\\theta] \\\\\n",
    "&= \\mu \\\\ \n",
    "&= \\eta \\\\\n",
    "&= \\theta^Tx\n",
    "\\end{align}\n",
    "$$\n",
    "</font>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;The first equality follows Assumption 2, above.  \n",
    "&emsp;&emsp;The second equality follows from the fact that $y|x;\\theta\\sim\\mathcal{N}(\\mu,\\sigma^2)$, and so its expected value is given by $\\mu$.  \n",
    "&emsp;&emsp;The third equality follows from Assumption 1 (and our earlier derivation showing that $\\mu=\\eta$ in the formulation of the Gaussian as an exponential family distribution)  \n",
    "&emsp;&emsp;The last equality follows from Assumption 3.\n",
    "</font>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2.3 Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;We now consider logistic regression. Here we are interested in binary classification, so $y\\in\\{0, 1\\}$. Given that $y$ is binary-valued, it therefore seems natural to choose the Bernoulli family of distributions to model the conditional distribution of $y$ given $x$. In our formulation of the Bernoulli distribution as an exponential family distribution, we had $\\phi=\\frac{1}{1+e^{-\\eta}}$. Furthermore, note that if $y|x;\\theta\\sim{Bernoulli(\\phi)}$, then $E[y|x;\\theta]=\\phi$. So, following a similar derivation as the one for ordinary least squares, we get:\n",
    "</font>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "$$\n",
    "\\begin{align}\n",
    "h_\\theta(x) &= E[y|x;\\theta] \\\\\n",
    "&= \\phi \\\\\n",
    "&= \\frac{1}{1+e^{-\\eta}} \\\\\n",
    "&= \\frac{1}{1+e^{-\\theta^Tx}}\n",
    "\\end{align}\n",
    "$$\n",
    "</font>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;So, this gives us hypothesis functions of the form $h_\\theta(x)=\\frac{1}{1+e^{-\\theta^Tx}}$. If you are previously wondering how we came up with the form of the logistic function $\\frac{1}{1+e^{-z}}$, this gives one answer: Once we assume that $y$ conditioned on $x$ is Bernoulli, it arises as a consequence of the definition of GLMs and exponential family distributions.\n",
    "</font>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
