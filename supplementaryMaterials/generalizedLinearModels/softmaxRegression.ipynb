{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "本文主要参考了以下材料：  \n",
    "1. cs229: *9.3 softmax regression*  \n",
    "2. http://ufldl.stanford.edu/wiki/index.php/Softmax%E5%9B%9E%E5%BD%92  \n",
    "3. https://blog.csdn.net/u012328159/article/details/72155874\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Regression\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 原理推导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;Consider a classification problem in which the response variable $y$ can take on any one of $k$ values, so $y\\in\\{1,\\,2,..., k\\}$. The response variable is still discrete, but can now take on more than two values. We will thus model it as distributed according to a multinomial distribution.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;Lets derive a GLM for modelling this type of multinomial data. To do so, we will begin by expressing the multinomial as an exponential family distribution.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;To parameterize a multinomial over $k$ possible outcomes, one could use $k$ parameters $\\phi_1,...,\\phi_k$ specifying the probability of each of the outcomes. However, these parameters would be redundant, or more formally, they would not be independent (since knowing any $k-1$ of the $\\phi_i$'s uniquely determines the last one, as they must satisfy $\\sum^{k}_{i=1}\\phi_i=1$). So, we will instead parameterize the multinomial with only $k-1$ parameters, $\\phi_1,...,\\phi_{k-1}$, where $\\phi_i=p(y=i;\\phi)$, and $p(y=k;\\phi)=1-\\sum^{k-1}_{i=1}\\phi_i$. For notational convenience, we will also let $\\phi_k=1-\\sum^{k-1}_{i=1}\\phi_i$, but we should keep in mind that this is not a parameter, and that it is fully specified by $\\phi_1,...,\\phi_{k-1}$.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;To express the multinomial as an exponential family distribution, we will define $T(y)\\in\\mathbb{R}^{k-1}$ as follows:\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "$$\n",
    "T(1)=\\begin{bmatrix}1 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\0\\end{bmatrix},\n",
    "T(2)=\\begin{bmatrix}0 \\\\ 1 \\\\ 0 \\\\ \\vdots \\\\0\\end{bmatrix},\n",
    "T(3)=\\begin{bmatrix}0 \\\\ 0 \\\\ 1 \\\\ \\vdots \\\\0\\end{bmatrix},\\cdots,\n",
    "T(k-1)=\\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\1\\end{bmatrix},\n",
    "T(k)=\\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\0\\end{bmatrix},\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;Unlike our previous examples, here we do not have $T(y)=y$; also, $T(y)$ is now a $k-1$ dimensional vector, rather than a real number. We will write $(T(y))_i$ to denote the $i$-th element of the vector $T(y)$. \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;We introduce one more very useful piece of notation. An indicator function $1\\{\\cdot\\}$ takes on a value of 1 if its argument is true, and 0 otherwise. So, we can write the relationship between $T(y)$ and $y$ as $(T(y))_i=1\\{y=i\\}$&ensp;(当且仅当$y=i$时，向量$T(y)$的第$i$个位置元素为1). Further, we have that $E[(T(y))_i]=P(y=i)=\\phi_i$.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;We are now ready to show that the multinomial is a member of the exponential family. We have:\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "$$\n",
    "\\begin{align}\n",
    "p(y;\\phi) &= \\phi_1^{1\\{y=1\\}}\\phi_2^{1\\{y=2\\}}\\cdots\\phi_k^{1\\{y=k\\}} \\\\\n",
    "&= \\phi_1^{1\\{y=1\\}}\\phi_2^{1\\{y=1\\}}\\cdots\\phi_k^{1-\\sum^{k-1}_{i=1}1\\{y=i\\}} \\\\\n",
    "&= \\phi_1^{(T(y))_1}\\phi_2^{(T(y))_2}\\cdots\\phi_k^{1-\\sum^{k-1}_{i=1}(T(y))_i} \\\\\n",
    "&= exp((T(y))_1log(\\phi_1)+(T(y))_2log(\\phi_2)+\\cdots+(1-\\sum^{k-1}_{i=1}(T(y))_i)log(\\phi_k)) \\\\\n",
    "&= exp((T(y))_1log(\\frac{\\phi_1}{\\phi_k})+(T(y))_2log(\\frac{\\phi_2}{\\phi_k})+\\cdots+(T(y))_{k-1}log(\\frac{\\phi_{k-1}}{\\phi_k})+log(\\phi_k)) \\\\\n",
    "&= b(y)\\,exp(\\eta^TT(y)-a(\\eta))\n",
    "\\end{align}\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;where\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "$$\n",
    "\\eta=\\begin{bmatrix} log(\\frac{\\phi_1}{\\phi_k}) \\\\ log(\\frac{\\phi_2}{\\phi_k}) \\\\ \\vdots \\\\ log(\\frac{\\phi_{k-1}}{\\phi_k}) \\end{bmatrix}\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "$$a(\\eta) = -log(\\phi_k)$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "$$b(y)=1$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;This completes our formulation of the multinomial as an exponential family distribution.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;The link function is given (for $i=1,\\cdots,k$) by\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "$$\n",
    "\\eta_i=log\\frac{\\phi_i}{\\phi_k}\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;For convenience, we have also defined $\\eta_k=log(\\frac{\\phi_k}{\\phi_k})=0$. To invert the link function and derive the response function, we therefore have that\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "$$\n",
    "e^{\\eta_i}=\\frac{\\phi_i}{\\phi_k}\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "$$\n",
    "\\phi_ke^{\\eta_i}=\\phi_i\\quad-(1)\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "$$\n",
    "\\phi_k\\sum_{i=1}^{k}e^{\\eta_i}=\\sum_{i=1}^{k}\\phi_i=1\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;This implies that $\\phi_k={1}/{\\sum_{i=1}^{k}e^{\\eta_i}}$, which can be substituted back into Equation (1) to give the response function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "$$\n",
    "\\phi_i=\\frac{e^{\\eta_i}}{\\sum_{l=1}^{k}e^{\\eta_l}}\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;This function mapping from the $\\eta$'s to the $\\phi$'s is called the **softmax function**.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;To complete our model, we use Assumption 3, given earlier, that the $\\eta$'s are linearly related to the $x$'s. So, have $\\eta_i=\\theta^T_ix$ (for $i\\ =\\ 1,\\cdots,k-1$), where $\\theta_1,\\cdots,\\theta_{k-1}\\in\\mathbb{R}^{n+1}$ are the parameters of our model. For notational convenience, we can also define $\\theta_k=0$, so that $\\eta_k=\\theta_k^Tx=0$, as given previously. Hence, our model assumes that the conditional distribution of $y$ given $x$ is given by\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "$$\n",
    "\\begin{align}\n",
    "p(y=i|x;\\theta) &= \\phi_i \\\\\n",
    "&= \\frac{e^{\\eta_i}}{\\sum_{l=1}^{k}e^{\\eta_l}} \\\\\n",
    "&= \\frac{e^{\\theta^T_ix}}{\\sum_{l=1}^{k}e^{\\theta^T_lx}} \\quad-(2)\n",
    "\\end{align}\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;This model, which applies to classification problems where $y\\in\\{1,\\cdots,k\\}$, is called **softmax regression**. It is a generalization of logistic regression.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;Our hypothesis will output\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "$$\n",
    "\\begin{align}\n",
    "h_\\theta(x) &= E[T(y)\\,|\\,x;\\theta] \\\\\n",
    "&= E\\left[ {\\begin{array}{c|c}\\begin{matrix} 1\\{y=1\\} \\\\ 1\\{y=2\\} \\\\ \\cdots \\\\ 1\\{y=k-1\\} \\end{matrix}&\\begin{matrix} \\\\ \\\\ x;\\theta \\\\ \\\\ \\end{matrix}\\end{array}} \\right] \\\\\n",
    "&= \\begin{bmatrix} \\phi_1 \\\\ \\phi_2 \\\\ \\cdots \\\\ \\phi_{k-1} \\end{bmatrix} \\\\\n",
    "&= \\begin{bmatrix} \\frac{exp(\\theta^T_1x)}{\\sum^{k}_{l=1}exp(\\theta^T_lx)} \\\\ \\frac{exp(\\theta^T_2x)}{\\sum^{k}_{l=1}exp(\\theta^T_lx)} \\\\ \\vdots \\\\ \\frac{exp(\\theta^T_{k-1}x)}{\\sum^{k}_{l=1}exp(\\theta^T_lx)} \\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;In other words, our hypothesis will output the estimated probability that $p(y=i|x;\\theta)$, for every value of $i=1,\\cdots,k$.(Even though $h_\\theta(x)$ as defined above is only $k-1$ dimensional, clearly $p(y=k|x;\\theta)$ can be obtained as $1-\\sum^{k-1}_{i=1}\\phi_i$.)\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;Lastly, lets discuss parameter fitting. Similar to our original derivation of ordinary least squares and logistic regression, if we have a training set of $m$ examples $\\{(x^{(i)},y^{(i)});i=1,\\cdots,m\\}$ and would like to learn the parameter $\\theta_i$ of this model, we would begin by writing down the log-likelihood:\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "$$\n",
    "\\begin{align}\n",
    "l(\\theta) &= \\sum^{m}_{i=1}logp(y^{(i)}|x^{(i)};\\theta) \\\\\n",
    "&= \\sum^{m}_{i=1}log\\prod^{k}_{j=1}\\left( \\frac{e^{\\theta^T_jx^{(i)}}}{\\sum^{k}_{i=l}e^{\\theta^T_lx^{(i)}}} \\right)^{1\\{y^{(i)}=j\\}} \\\\\n",
    "&= \\sum^{m}_{i=1}\\sum^{k}_{j=1}1\\{y^{(i)}=j\\}log\\frac{e^{\\theta^T_jx^{(i)}}}{\\sum^{k}_{l=1}e^{\\theta^T_lx^{(i)}}}\n",
    "\\end{align} \n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;To obtain the second line above, we used the definition for $p(y|x;\\theta)$ given in Equation (2). We can now obtain the maximum likelihood estimate of the parameters by maximizing $l(\\theta)$ in terms of $\\theta$, using a method such as gradient ascent or Newton's method.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 梯度下降法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;首先，定义\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "$$\n",
    "x^{(i)}=\\begin{bmatrix} x^{(i)}_0 \\\\ x^{(i)}_1 \\\\ \\vdots \\\\ x^{(i)}_{n+1} \\end{bmatrix}_{(n+1)×1} \\quad\n",
    "X=\\begin{bmatrix} -(x^{(1)})^T- \\\\ -(x^{(2)})^T- \\\\ \\cdots \\\\ -(x^{(m)})^T- \\end{bmatrix}_{m×(n+1)} \\quad\n",
    "\\theta=\\begin{bmatrix} -\\theta^T_1- \\\\ -\\theta^T_2- \\\\ \\cdots \\\\ -\\theta^T_k- \\end{bmatrix}_{k×(n+1)}\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;注意到，$x^{(i)}\\in\\mathbb{R}^{n+1}$，其中定义$x^{(i)}_0=0$。然后，$\\theta_1,\\theta_2,\\cdots,\\theta_k\\in\\mathbb{R}^{n+1}$，在这里，我没有定义$\\theta_k=0$，因此，存在过度参数化的问题。所以，在接下来的小节内会对这个问题进行解决。最后，$y\\in\\{1,2,\\cdots,k\\}$。\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;因此，我们定义Cost Function为\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "$$\n",
    "J(\\theta)=-\\frac{1}{m}\\left( \\sum^{m}_{i=1}\\sum^{k}_{j=1}1\\{y^{(i)}=j\\}log\\frac{e^{\\theta^T_jx^{(i)}}}{\\sum^{k}_{l=1}e^{\\theta^T_l}x} \\right)\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;值得注意的是，上述公式是逻辑回归的cost function的推广，逻辑回归的cost function可以改为：\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "$$\n",
    "\\begin{align}\n",
    "J(\\theta) &= -\\frac{1}{m} \\left( \\sum^{m}_{i=1}(1-y^{(i)})log(1-h_\\theta(x^{(i)}))+y^{(i)}logh_\\theta(x^{(i)}) \\right) \\\\\n",
    "&= -\\frac{1}{m}\\left( \\sum^{m}_{i=1}\\sum^{1}_{j=0}1\\{y^{(i)}=j\\}log\\,p(y^{(i)}=j\\,|\\,x^{(i)};\\theta) \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Softmax Regression 模型参数化的特点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;由于上一节，我们没有定义 $\\theta_k=0$，这使得softmax regression有一个“冗余”的参数集。虽然，定义 $\\theta_k=0$ 可以避免这个问题，但是这会使得在算法实现中没有那么简单清楚，而且，这个问题是可以得到解决的。接下来，我们对这个问题进行具体说明。\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;假设我们从参数向量 $\\theta_j$ 中减去了向量 $\\psi$，这时，每一个 $\\theta_j$ 都变成了 $\\theta_j-\\psi \\ (j=1,\\cdots,k)$。此时假设函数变成了以下的式子：\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "$$\n",
    "\\begin{align}\n",
    "p(y^{(i)}=j\\,|\\,x^{(i)};\\theta)\n",
    "&= \\frac{e^{(\\theta_j-\\psi)^Tx^{(i)}}}{\\sum^{k}_{l=1}e^{(\\theta_l-\\psi)^Tx^{(i)}}} \\\\ \n",
    "& = \\frac{e^{\\theta^T_jx^{(i)}}e^{-\\psi^Tx^{(i)}}}{\\sum^{k}_{l=1}e^{\\theta_l^Tx^{(i)}}e^{-\\psi^Tx^{(i)}}} \\\\\n",
    "&= \\frac{e^{\\theta_j^Tx^{(i)}}}{\\sum^{k}_{l=1}e^{\\theta_l^T}x^{(i)}}\n",
    "\\end{align}\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;换句话说，从 $\\theta_j$ 中减去 $\\psi$ 完全不影响假设函数的预测结果。这表明前面的softmax regression模型中存在冗余的参数。更正式一点说，softmax regression模型被过度参数化了。对于任意一个用于拟合数据的假设函数，可以求出多组参数值，这些参数得到的是完全相同的假设函数 $h_\\theta$。\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;进一步而言，如果参数 $(\\theta_1,\\theta_2,\\cdots,\\theta_k)$ 是 cost function $J(\\theta)$ 的极小值点，那么 $(\\theta_1-\\psi,\\theta_2-\\psi,\\cdots,\\theta_k-\\psi)$ 同样也是它的极小值点，其中 $\\psi$ 可以为任意向量。因此，使得 $J(\\theta)$ 最小化的解不是唯一的。（有趣的是，由于 $J(\\theta)$仍然是一个凸函数，因此梯度下降不会遇到局部最优解的问题，但是 $Hessian$ 矩阵是奇异的/不可逆的，这会直接导致采用牛顿法优化就遇到数值计算的问题）\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;注意，当 $\\phi=\\theta_k$ 时，我们总是可以将 $\\theta_k$ 替换为 $\\theta_k-\\psi=\\vec{0}$（即替换为全零向量），并且这种替换不会影响假设函数。因此，我们可以去掉参数向量 $\\theta_k$ （或者其他 $\\theta_j$ 中的任意一个）而不影响假设函数的表达能力。实际上，与其优化全部的 $k×(n+1)$ 个参数 $(\\theta_1,\\theta_2,\\cdots,\\theta_k)$ （其中，$\\theta_j\\in\\mathbb{R}^{n+1}$），我们可以令 $\\theta_k=\\vec{0}$，只优化剩余的 $(k-1)×(n+1)$ 个参数，这样算法依然能够正常工作。\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;在实际应用中，为了让算法实现更加简单清楚，往往保留所有参数 $(\\theta_1,\\theta_2,\\cdots,\\theta_k)$，而不任意地将某一参数设置为 $\\vec{0}$。但此时，我们需要对 cost function 做一个改动：加入权重衰减项。权重衰减项可以解决 softmax regression 参数冗余所带来的数值问题。\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 权重衰减"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;我们通过添加一个权重衰减项 $\\frac{\\lambda}{2}\\sum^{k}_{i=1}\\sum^{n}_{j=0}\\theta_{ij}^2$ 来修改 cost function，这个衰减项会惩罚过大的参数值，现在我们的 cost function 变为：\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "$$\n",
    "J(\\theta)=-\\frac{1}{m}\\left( \\sum^{m}_{i=1}\\sum^{k}_{j=1}1\\{y^{(i)}=j\\}\\,log\\,\\frac{e^{\\theta_j^Tx^{(i)}}}{\\sum^{k}_{l=1}e^{\\theta^T_lx^{(i)}}} \\right)+\\frac{\\lambda}{2}\\sum^{k}_{i=1}\\sum^{n}_{j=0}\\theta_{ij}^2\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;有了这个权重衰减项之后($\\lambda>0$)，Cost function 就变成了严格的凸函数，这样就可以保证得到唯一的解了。此时的 Hessian 矩阵变成可逆矩阵，并且因为 $J(\\theta)$ 是凸函数，梯度下降法和L-BFGS等算法可以保证收敛到全局最优解。\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;为了使用优化算法，我们需要求得这个新函数的 $J(\\theta)$ 的导数，如下：\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "$$\n",
    "\\triangledown_{\\theta_j}J(\\theta)=-\\frac{1}{m}\\left( x^{(i)}(1\\{y^{(i)}=j\\}-p(y^{(i)}\\,|\\,x^{(i)};\\theta)) \\right)+\\lambda\\theta_j\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;注意，这里的 $\\triangledown_{\\theta_j}J(\\theta)\\in\\mathbb{R}^{n+1}$。通过最小化 $J(\\theta)$，我们就能实现一个可用的 softmax regression 模型。\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2.4 推导 $\\frac{\\partial{J(\\theta)}}{\\partial\\theta_j}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;方法一：\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial{J(\\theta)}}{\\partial\\theta_j}\n",
    "&= -\\frac{1}{m}\\frac{\\partial}{\\partial\\theta_j}\\left[ \\sum^{m}_{i=1}\\sum^{k}_{j=1}1\\{y^{(i)}=j\\}\\,log\\,\\frac{e^{\\theta_j^Tx^{(i)}}}{\\sum^{k}_{l=1}e^{\\theta_l^Tx^{(i)}}} \\right]+\\lambda\\theta_j \\\\\n",
    "&= -\\frac{1}{m}\\frac{\\partial}{\\partial\\theta_j}\\left[ \\sum^{m}_{i=1}\\sum^{k}_{j=1}1\\{y^{(i)}=j\\}\\,(log\\,e^{\\theta_j^Tx^{(i)}}-log\\,\\sum^{k}_{l=1}e^{\\theta_l^Tx^{(i)}}) \\right]+\\lambda\\theta_j \\\\\n",
    "&= -\\frac{1}{m}\\frac{\\partial}{\\partial\\theta_j}\\left[ \\sum^{m}_{i=1}\\sum^{k}_{j=1}1\\{y^{(i)}=j\\}\\,(\\theta_j^Tx^{(i)}-log\\,\\sum^{k}_{l=1}e^{\\theta_l^Tx^{(i)}}) \\right]+\\lambda\\theta_j \\\\\n",
    "&= -\\frac{1}{m}\\frac{\\partial}{\\partial\\theta_j}\\left[ \\sum^{m}_{i=1}1\\{y^{(i)}=j\\}\\,(\\sum^{k}_{j=1}\\theta_j^Tx^{(i)}-\\sum^{k}_{j=1}log\\,\\sum^{k}_{l=1}e^{\\theta_l^Tx^{(i)}}) \\right]+\\lambda\\theta_j \\\\\n",
    "&= -\\frac{1}{m}\\left[ \\sum^{m}_{i=1}1\\{y^{(i)}=j\\}\\,(x^{(i)}-\\sum^{k}_{j=1}\\frac{x^{(i)}e^{\\theta_j^Tx^{(i)}}}{\\sum^k_{l=1}e^{\\theta_l^Tx^{(i)}}}) \\right]+\\lambda\\theta_j \\\\\n",
    "&= -\\frac{1}{m}\\left[ \\sum^{m}_{i=1}x^{(i)}\\,(1\\{y^{(i)}=j\\}-\\sum^{k}_{j=1}1\\{y^{(i)}=j\\}\\frac{e^{\\theta_j^Tx^{(i)}}}{\\sum^k_{l=1}e^{\\theta_l^Tx^{(i)}}}) \\right]+\\lambda\\theta_j \\\\\n",
    "&= -\\frac{1}{m}\\left[ \\sum^{m}_{i=1}x^{(i)}\\,(1\\{y^{(i)}=j\\}-\\frac{e^{\\theta_j^Tx^{(i)}}}{\\sum^k_{l=1}e^{\\theta_l^Tx^{(i)}}}) \\right]+\\lambda\\theta_j \\\\\n",
    "&= -\\frac{1}{m}\\left[ \\sum^{m}_{i=1}x^{(i)}\\,(1\\{y^{(i)}=j\\}-p(y^{(i)}\\,|\\,x^{(i)};\\theta)) \\right]+\\lambda\\theta_j\n",
    "\\end{align} \n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;方法二：\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial{J(\\theta)}}{\\partial\\theta_j}\n",
    "&= -\\frac{1}{m}\\left[ \\sum^{m}_{i=1}\\frac{\\partial}{\\partial{\\theta_j}}(1\\{y^{(i)}=j\\}\\,log\\,\\frac{e^{\\theta_j^Tx^{(i)}}}{\\sum^k_{l=1}e^{\\theta_l^Tx^{(i)}}}+\\sum^k_{c\\ne{j}}1\\{y^{(i)}=c\\}\\,log\\,\\frac{e^{\\theta_c^Tx^{(i)}}}{\\sum^k_{l=1}e^{\\theta_l^Tx^{(i)}}}) \\right]+\\lambda\\theta_j \\\\ \n",
    "&= -\\frac{1}{m}\\left[ \\sum^{m}_{i=1}(1\\{y^{(i)}=j\\}(x^{(i)}-\\frac{x^{(i)}e^{\\theta_j^Tx^{(i)}}}{\\sum^k_{l=1}e^{\\theta_l^Tx^{(i)}}})+\\sum^k_{c\\ne{j}}1\\{y^{(i)}=c\\}(-\\frac{x^{(i)}e^{\\theta_j^Tx^{(i)}}}{\\sum^k_{l=1}e^{\\theta_l^Tx^{(i)}}})) \\right]+\\lambda\\theta_j \\\\\n",
    "&= -\\frac{1}{m}\\left[ \\sum^{m}_{i=1}(x^{(i)}1\\{y^{(i)}=j\\}(1-\\frac{e^{\\theta_j^Tx^{(i)}}}{\\sum^k_{l=1}e^{\\theta_l^Tx^{(i)}}})+\\sum^k_{c\\ne{j}}1\\{y^{(i)}=c\\}(-\\frac{x^{(i)}e^{\\theta_j^Tx^{(i)}}}{\\sum^k_{l=1}e^{\\theta_l^Tx^{(i)}}})) \\right]+\\lambda\\theta_j \\\\\n",
    "&= \\frac{1}{m}\\left[ \\sum^m_{i=1}x^{(i)}(1\\{y^{(i)}=j\\}-1\\{y^{(i)}=j\\}p(y^{(i)}\\,|\\,x^{(i)};\\theta))+\\sum^k_{c\\ne{j}}1\\{y^{(i)}=c\\}(-p(y^{(i)}\\,|\\,x^{(i)};\\theta)) \\right]+\\lambda\\theta_j \\\\\n",
    "&= \\frac{1}{m}\\left[ \\sum^m_{i=1}x^{(i)}(1\\{y^{(i)}=j\\}-\\sum^k_{j=1}1\\{y^{(i)}=j\\}p(y^{(i)}\\,|\\,x^{(i)};\\theta) \\right]+\\lambda\\theta_j \\\\\n",
    "&= -\\frac{1}{m}\\left[ \\sum^{m}_{i=1}x^{(i)}\\,(1\\{y^{(i)}=j\\}-p(y^{(i)}\\,|\\,x^{(i)};\\theta)) \\right]+\\lambda\\theta_j\n",
    "\\end{align}\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2.5 矩阵化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;因为 $y\\in\\{1,2,\\cdots,k\\}$，所以，对 $y$ 进行独热编码。即 $y^{(i)}\\in\\mathbb{R}^{k}$，其中，若 $y^{(i)}$ 属于类别 $i$，则 $y^{(i)}$ 第 $i$ 个位置上的元素为1，其余位置元素为0。因此，定义矩阵 $G$ 为\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "$$\n",
    "G=\n",
    "\\begin{bmatrix}\n",
    "-(y^{(1)})^T- \\\\ -(y^{(2)})^T- \\\\ \\vdots \\\\ -(y^{(m)})^T-\n",
    "\\end{bmatrix}_{m×k}\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;定义概率矩阵 $P$ 为\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "$$\n",
    "P_{m×k} = norm(\\,exp(X_{m×(n+1)}\\cdot\\theta_{(n+1)×k}^T)\\,)\\quad\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;其中，norm 表示归一化项，因此，概率矩阵 $P$ 的具体计算方式为：首先，计算 $exp(X\\theta^T)$ 得到 $m×k$ 的矩阵。其次，使用 $np.sum()$ 对该该矩阵按行进行求和，得到 m×1 的矩阵。最后，利用Python的广播（broadcast）机制，将该矩阵与 $exp(X\\theta^T)$ 对应位置元素进行相乘（element-wise multiplication）得到概率矩阵 $P$。\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;于是\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "$$\n",
    "\\frac{\\partial{J(\\theta)}}{\\partial\\theta}\n",
    "=-\\frac{1}{m}(G-P)^T\\cdot{X}+\\lambda\\theta\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;所以，cost function为\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "$$\n",
    "J(\\theta)=-np.mean(G\\circ{P})+\\lambda \\ {np.sum(\\theta)}\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;其中，$\\circ$ 表示对应位置元素相乘，即 element-wise multiplication。\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3. 实现 softmax regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.1 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>traget</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                5.1               3.5                1.4               0.2   \n",
       "1                4.9               3.0                1.4               0.2   \n",
       "2                4.7               3.2                1.3               0.2   \n",
       "3                4.6               3.1                1.5               0.2   \n",
       "4                5.0               3.6                1.4               0.2   \n",
       "\n",
       "   traget  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "features = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "label = pd.DataFrame(data=iris.target, columns=['traget'])\n",
    "data = pd.concat([features, label], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadData(df):\n",
    "    ones = pd.DataFrame({'ones': np.ones(len(df))})\n",
    "    df = pd.concat([ones, df], axis=1)\n",
    "    X = df.iloc[:,:-1].values\n",
    "    y = df.iloc[:,-1].values\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150, 5), (150,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = loadData(data)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.2 数据可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAs4AAAHvCAYAAABE9FkiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8cc41bf6d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(X[:, 3][y==0], X[:, 4][y==0], marker='x', label=iris.target_names[0])\n",
    "plt.scatter(X[:, 3][y==1], X[:, 4][y==1], marker='^', label=iris.target_names[1])\n",
    "plt.scatter(X[:, 3][y==2], X[:, 4][y==2], marker='+', label=iris.target_names[2])\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel(iris.feature_names[2])\n",
    "plt.ylabel(iris.feature_names[3])\n",
    "plt.title('Iris Data Set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Softmax Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;对 $y$ 进行独热编码，得到矩阵G。\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def oneHotY(y):\n",
    "    # m为样本数\n",
    "    m = y.shape[0]\n",
    "    # k为类别数\n",
    "    k = len(np.unique(y))\n",
    "    \n",
    "    oneHotY = np.zeros((m, k))\n",
    "    for i in range(k):\n",
    "        oneHotY[:, i] = (y==i)\n",
    "        \n",
    "    return oneHotY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G = oneHotY(y)\n",
    "G.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initializeWithZeros(X, y):\n",
    "    k = len(np.unique(y))\n",
    "    \n",
    "    return np.zeros((k, X.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def probabilityMatrix(X, theta):\n",
    "    expScore = np.exp(X @ theta.T)\n",
    "    sumScore = np.sum(expScore, axis=1).reshape(-1, 1)\n",
    "    \n",
    "    return np.multiply(expScore, sumScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeCost(X, G, theta, l):\n",
    "    P = probabilityMatrix(X, theta)\n",
    "    return -np.mean(np.multiply(G, np.log(P))) + l * theta.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeGradient(X, G, theta, l):\n",
    "    m = X.shape[0]\n",
    "    P = probabilityMatrix(X, theta)\n",
    "    grad = -((G-P).T @ X) / m + l *theta\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def batchGradientDescent(X, G, theta, alpha, iters, l, printFlag=True):\n",
    "    costs = np.zeros(iters)\n",
    "    \n",
    "    for i in range(iters):\n",
    "        theta = theta - alpha * computeGradient(X, G, theta, l)\n",
    "        costs[i] = computeCost(X, G, theta, l)\n",
    "        \n",
    "        if printFlag and i % 1000 == 0:\n",
    "            print(costs[i])\n",
    "        \n",
    "    return theta, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(X, theta):\n",
    "    P = probabilityMatrix(X, theta)\n",
    "    \n",
    "    return np.argmax(P, axis=1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.577650282651\n",
      "0.0685942497214\n",
      "0.062683569659\n",
      "0.0612484292857\n",
      "0.0608645968918\n",
      "0.0607601270249\n",
      "0.060731296824\n",
      "0.0607232213633\n",
      "0.0607209225948\n",
      "0.0607202570216\n"
     ]
    }
   ],
   "source": [
    "X, y = loadData(data)\n",
    "G = oneHotY(y)\n",
    "theta = initializeWithZeros(X, y)\n",
    "\n",
    "iters = 10000\n",
    "alpha = 0.01\n",
    "l = 0.1\n",
    "\n",
    "theta, costs = batchGradientDescent(X, G, theta, alpha, iters, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAs4AAAHvCAYAAABE9FkiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niSpoe8ixgAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8cf8540898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(np.arange(iters), costs, color='red')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Error vs. Training Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        50\n",
      "          1       1.00      0.80      0.89        50\n",
      "          2       0.83      1.00      0.91        50\n",
      "\n",
      "avg / total       0.94      0.93      0.93       150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict(X, theta)\n",
    "print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4. Softmax Regression 与 Logistic Regression 的关系"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;当类别数 $k=2$ 时，softmax regression 退化为 logistic regression。这表明 softmax regression 是 logistic regression 的一般形式。具体地说，当 $k=2$ 时，softmax regression 的假设函数为：\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "$$\n",
    "h_\\theta(x)=\\frac{1}{e^{\\theta_1^Tx}+e^{\\theta_2^T}x}\n",
    "\\begin{bmatrix}\n",
    "e^{\\theta_1^Tx} \\\\\n",
    "e^{\\theta_2^Tx}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;利用 softmax regression 回归参数冗余的特点，我们令 $\\psi=\\theta_1$，并且从两个参数向量中都减去向量 $\\theta_1$，得到：\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "$$\n",
    "\\begin{align}\n",
    "h(x) &= \\frac{1}{e^{\\vec{0}^Tx}+e^{(\\theta_2-\\theta_1)^Tx}}\n",
    "\\begin{bmatrix}\n",
    "e^{\\vec{0}^Tx} \\\\\n",
    "e^{(\\theta_1-\\theta_2)^Tx}\n",
    "\\end{bmatrix} \\\\\n",
    "&= \n",
    "\\begin{bmatrix}\n",
    "\\frac{1}{1+e^{(\\theta_1-\\theta_2)^Tx}} \\\\\n",
    "\\frac{e^{(\\theta_1-\\theta_2)^Tx}}{1+e^{(\\theta_1-\\theta_2)^Tx}}\n",
    "\\end{bmatrix} \\\\\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "\\frac{1}{1+e^{(\\theta_1-\\theta_2)^Tx}} \\\\\n",
    "1-\\frac{1}{1+e^{(\\theta_1-\\theta_2)^Tx}}\n",
    "\\end{bmatrix} \n",
    "\\end{align}\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;因此，用 $\\theta'$ 来表示 $\\theta_1-\\theta_2$，我们就会发现 softmax regression 预测其中一个类别的概率为 $\\frac{1}{1+e^{(\\theta')^Tx}}$，另一个类别的概率为 $1-\\frac{1}{1+e^{(\\theta')^Tx}}$，这与logistic regression 是一致的。\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Softmax Regression vs. k 个二元分类器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;如果你在开发一个音乐分类的应用，需要对 $k$ 种类型的音乐进行识别，那么是选择使用 softmax regression，还是使用 logistic regression 建立 $k$ 个独立的二元分类器呢？\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;这一选择取决于你的类别之间是否互斥，例如，如果你有四个类别的音乐，分别为：古典音乐、乡村音乐、摇滚乐和爵士乐，那么你可以假设每个训练样本只会被打上一个标签（即一首歌只能属于这四种音乐类型的其中一种），此时，你应该使用类别数 $k=4$ 的 softmax regression（如果在你的数据集中，有的歌曲不属于以上四类的其中任何一类，那么你可以添加一个”其他类“，并将类别数 $k$ 设为5）。\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;如果你的四个类别如下：人声音乐、舞曲、影视原声、流行歌曲，那么这些类别之间并不是互斥的。例如：一首歌曲可以来源于影视原声，同时也包含人声。在这种情况下，使用4个二分类的 logistic regression 更为合适。这样每个新的音乐作品，我们的算法可以分别判断它是否属于各个类别。\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;现在，我们来看一个计算机视觉领域的例子，你的任务是将图像分到三个不同的类别中。（1）假设这三个类别分别是：室内场景、户外城区场景、户外荒野场景。你会使用softmax regression 还是3个 logistic regression 呢？（2）现在假设这三个类别分别是室内场景、黑白图片、包含人物的图片，你会选择 softmax regression 还是多个 logistic regression 呢？\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;在第一个例子中，三个类别是互斥的，因此，更适于选择 softmax regression。而在第二个例子中，建立三个独立的 logistic regression 更加合适。\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
