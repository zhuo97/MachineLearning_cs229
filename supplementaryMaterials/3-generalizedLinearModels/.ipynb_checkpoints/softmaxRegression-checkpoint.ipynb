{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "本文主要参考了以下材料：  \n",
    "1. cs229: *9.3 softmax regression*  \n",
    "2. http://ufldl.stanford.edu/wiki/index.php/Softmax%E5%9B%9E%E5%BD%92  \n",
    "3. https://blog.csdn.net/u012328159/article/details/72155874\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Regression\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 原理推导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;Consider a classification problem in which the response variable $y$ can take on any one of $k$ values, so $y\\in\\{1,\\,2,..., k\\}$. The response variable is still discrete, but can now take on more than two values. We will thus model it as distributed according to a multinomial distribution.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;Lets derive a GLM for modelling this type of multinomial data. To do so, we will begin by expressing the multinomial as an exponential family distribution.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;To parameterize a multinomial over $k$ possible outcomes, one could use $k$ parameters $\\phi_1,...,\\phi_k$ specifying the probability of each of the outcomes. However, these parameters would be redundant, or more formally, they would not be independent (since knowing any $k-1$ of the $\\phi_i$'s uniquely determines the last one, as they must satisfy $\\sum^{k}_{i=1}\\phi_i=1$). So, we will instead parameterize the multinomial with only $k-1$ parameters, $\\phi_1,...,\\phi_{k-1}$, where $\\phi_i=p(y=i;\\phi)$, and $p(y=k;\\phi)=1-\\sum^{k-1}_{i=1}\\phi_i$. For notational convenience, we will also let $\\phi_k=1-\\sum^{k-1}_{i=1}\\phi_i$, but we should keep in mind that this is not a parameter, and that it is fully specified by $\\phi_1,...,\\phi_{k-1}$.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;To express the multinomial as an exponential family distribution, we will define $T(y)\\in\\mathbb{R}^{k-1}$ as follows:\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "$$\n",
    "T(1)=\\begin{bmatrix}1 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\0\\end{bmatrix},\n",
    "T(2)=\\begin{bmatrix}0 \\\\ 1 \\\\ 0 \\\\ \\vdots \\\\0\\end{bmatrix},\n",
    "T(3)=\\begin{bmatrix}0 \\\\ 0 \\\\ 1 \\\\ \\vdots \\\\0\\end{bmatrix},\\cdots,\n",
    "T(k-1)=\\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\1\\end{bmatrix},\n",
    "T(k)=\\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\0\\end{bmatrix},\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;Unlike our previous examples, here we do not have $T(y)=y$; also, $T(y)$ is now a $k-1$ dimensional vector, rather than a real number. We will write $(T(y))_i$ to denote the $i$-th element of the vector $T(y)$. \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;We introduce one more very useful piece of notation. An indicator function $1\\{\\cdot\\}$ takes on a value of 1 if its argument is true, and 0 otherwise. So, we can write the relationship between $T(y)$ and $y$ as $(T(y))_i=1\\{y=i\\}$&ensp;(当且仅当$y=i$时，向量$T(y)$的第$i$个位置元素为1). Further, we have that $E[(T(y))_i]=P(y=i)=\\phi_i$.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;We are now ready to show that the multinomial is a member of the exponential family. We have:\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "$$\n",
    "\\begin{align}\n",
    "p(y;\\phi) &= \\phi_1^{1\\{y=1\\}}\\phi_2^{1\\{y=2\\}}\\cdots\\phi_k^{1\\{y=k\\}} \\\\\n",
    "&= \\phi_1^{1\\{y=1\\}}\\phi_2^{1\\{y=1\\}}\\cdots\\phi_k^{1-\\sum^{k-1}_{i=1}1\\{y=i\\}} \\\\\n",
    "&= \\phi_1^{(T(y))_1}\\phi_2^{(T(y))_2}\\cdots\\phi_k^{1-\\sum^{k-1}_{i=1}(T(y))_i} \\\\\n",
    "&= exp((T(y))_1log(\\phi_1)+(T(y))_2log(\\phi_2)+\\cdots+(1-\\sum^{k-1}_{i=1}(T(y))_i)log(\\phi_k)) \\\\\n",
    "&= exp((T(y))_1log(\\frac{\\phi_1}{\\phi_k})+(T(y))_2log(\\frac{\\phi_2}{\\phi_k})+\\cdots+(T(y))_{k-1}log(\\frac{\\phi_{k-1}}{\\phi_k})+log(\\phi_k)) \\\\\n",
    "&= b(y)\\,exp(\\eta^TT(y)-a(\\eta))\n",
    "\\end{align}\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;where\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "$$\n",
    "\\eta=\\begin{bmatrix} log(\\frac{\\phi_1}{\\phi_k}) \\\\ log(\\frac{\\phi_2}{\\phi_k}) \\\\ \\vdots \\\\ log(\\frac{\\phi_{k-1}}{\\phi_k}) \\end{bmatrix}\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "$$a(\\eta) = -log(\\phi_k)$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "$$b(y)=1$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;This completes our formulation of the multinomial as an exponential family distribution.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;The link function is given (for $i=1,\\cdots,k$) by\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "$$\n",
    "\\eta_i=log\\frac{\\phi_i}{\\phi_k}\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;For convenience, we have also defined $\\eta_k=log(\\frac{\\phi_k}{\\phi_k})=0$. To invert the link function and derive the response function, we therefore have that\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "$$\n",
    "e^{\\eta_i}=\\frac{\\phi_i}{\\phi_k}\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "$$\n",
    "\\phi_ke^{\\eta_i}=\\phi_i\\quad-(1)\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "$$\n",
    "\\phi_k\\sum_{i=1}^{k}e^{\\eta_i}=\\sum_{i=1}^{k}\\phi_i=1\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;This implies that $\\phi_k={1}/{\\sum_{i=1}^{k}e^{\\eta_i}}$, which can be substituted back into Equation (1) to give the response function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "$$\n",
    "\\phi_i=\\frac{e^{\\eta_i}}{\\sum_{l=1}^{k}e^{\\eta_l}}\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;This function mapping from the $\\eta$'s to the $\\phi$'s is called the **softmax function**.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;To complete our model, we use Assumption 3, given earlier, that the $\\eta$'s are linearly related to the $x$'s. So, have $\\eta_i=\\theta^T_ix$ (for $i\\ =\\ 1,\\cdots,k-1$), where $\\theta_1,\\cdots,\\theta_{k-1}\\in\\mathbb{R}^{n+1}$ are the parameters of our model. For notational convenience, we can also define $\\theta_k=0$, so that $\\eta_k=\\theta_k^Tx=0$, as given previously. Hence, our model assumes that the conditional distribution of $y$ given $x$ is given by\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "$$\n",
    "\\begin{align}\n",
    "p(y=i|x;\\theta) &= \\phi_i \\\\\n",
    "&= \\frac{e^{\\eta_i}}{\\sum_{l=1}^{k}e^{\\eta_l}} \\\\\n",
    "&= \\frac{e^{\\theta^T_ix}}{\\sum_{l=1}^{k}e^{\\theta^T_lx}} \\quad-(2)\n",
    "\\end{align}\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;This model, which applies to classification problems where $y\\in\\{1,\\cdots,k\\}$, is called **softmax regression**. It is a generalization of logistic regression.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;Our hypothesis will output\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "$$\n",
    "\\begin{align}\n",
    "h_\\theta(x) &= E[T(y)\\,|\\,x;\\theta] \\\\\n",
    "&= E\\left[ {\\begin{array}{c|c}\\begin{matrix} 1\\{y=1\\} \\\\ 1\\{y=2\\} \\\\ \\cdots \\\\ 1\\{y=k-1\\} \\end{matrix}&\\begin{matrix} \\\\ \\\\ x;\\theta \\\\ \\\\ \\end{matrix}\\end{array}} \\right] \\\\\n",
    "&= \\begin{bmatrix} \\phi_1 \\\\ \\phi_2 \\\\ \\cdots \\\\ \\phi_{k-1} \\end{bmatrix} \\\\\n",
    "&= \\begin{bmatrix} \\frac{exp(\\theta^T_1x)}{\\sum^{k}_{l=1}exp(\\theta^T_lx)} \\\\ \\frac{exp(\\theta^T_2x)}{\\sum^{k}_{l=1}exp(\\theta^T_lx)} \\\\ \\vdots \\\\ \\frac{exp(\\theta^T_{k-1}x)}{\\sum^{k}_{l=1}exp(\\theta^T_lx)} \\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;In other words, our hypothesis will output the estimated probability that $p(y=i|x;\\theta)$, for every value of $i=1,\\cdots,k$.(Even though $h_\\theta(x)$ as defined above is only $k-1$ dimensional, clearly $p(y=k|x;\\theta)$ can be obtained as $1-\\sum^{k-1}_{i=1}\\phi_i$.)\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;Lastly, lets discuss parameter fitting. Similar to our original derivation of ordinary least squares and logistic regression, if we have a training set of $m$ examples $\\{(x^{(i)},y^{(i)});i=1,\\cdots,m\\}$ and would like to learn the parameter $\\theta_i$ of this model, we would begin by writing down the log-likelihood:\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "$$\n",
    "\\begin{align}\n",
    "l(\\theta) &= \\sum^{m}_{i=1}logp(y^{(i)}|x^{(i)};\\theta) \\\\\n",
    "&= \\sum^{m}_{i=1}log\\prod^{k}_{j=1}\\left( \\frac{e^{\\theta^T_jx^{(i)}}}{\\sum^{k}_{i=l}e^{\\theta^T_lx^{(i)}}} \\right)^{1\\{y^{(i)}=j\\}} \\\\\n",
    "&= \\sum^{m}_{i=1}\\sum^{k}_{j=1}1\\{y^{(i)}=j\\}log\\frac{e^{\\theta^T_jx^{(i)}}}{\\sum^{k}_{l=1}e^{\\theta^T_lx^{(i)}}}\n",
    "\\end{align} \n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;To obtain the second line above, we used the definition for $p(y|x;\\theta)$ given in Equation (2). We can now obtain the maximum likelihood estimate of the parameters by maximizing $l(\\theta)$ in terms of $\\theta$, using a method such as gradient ascent or Newton's method.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 梯度下降法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;首先，定义\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "$$\n",
    "x^{(i)}=\\begin{bmatrix} x^{(i)}_0 \\\\ x^{(i)}_1 \\\\ \\vdots \\\\ x^{(i)}_{n+1} \\end{bmatrix}_{(n+1)×1} \\quad\n",
    "X=\\begin{bmatrix} -(x^{(1)})^T- \\\\ -(x^{(2)})^T- \\\\ \\cdots \\\\ -(x^{(m)})^T- \\end{bmatrix}_{m×(n+1)} \\quad\n",
    "\\theta=\\begin{bmatrix} -\\theta^T_1- \\\\ -\\theta^T_2- \\\\ \\cdots \\\\ -\\theta^T_k- \\end{bmatrix}_{k×(n+1)}\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;注意到，$x^{(i)}\\in\\mathbb{R}^{n+1}$，其中定义$x^{(i)}_0=0$。然后，$\\theta_1,\\theta_2,\\cdots,\\theta_k\\in\\mathbb{R}^{n+1}$，在这里，我没有定义$\\theta_k=0$，因此，存在过度参数化的问题。所以，在接下来的小节内会对这个问题进行解决。最后，$y\\in\\{1,2,\\cdots,k\\}$。\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;因此，我们定义Cost Function为\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "$$\n",
    "J(\\theta)=-\\frac{1}{m}\\left( \\sum^{m}_{i=1}\\sum^{k}_{j=1}1\\{y^{(i)}=j\\}log\\frac{e^{\\theta^T_jx^{(i)}}}{\\sum^{k}_{l=1}e^{\\theta^T_l}x} \\right)\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;值得注意的是，上述公式是逻辑回归的cost function的推广，逻辑回归的cost function可以改为：\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "$$\n",
    "\\begin{align}\n",
    "J(\\theta) &= -\\frac{1}{m} \\left( \\sum^{m}_{i=1}(1-y^{(i)})log(1-h_\\theta(x^{(i)}))+y^{(i)}logh_\\theta(x^{(i)}) \\right) \\\\\n",
    "&= -\\frac{1}{m}\\left( \\sum^{m}_{i=1}\\sum^{1}_{j=0}1\\{y^{(i)}=j\\}log\\,p(y^{(i)}=j\\,|\\,x^{(i)};\\theta) \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Softmax Regression 模型参数化的特点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;由于上一节，我们没有定义 $\\theta_k=0$，这使得softmax regression有一个“冗余”的参数集。虽然，定义 $\\theta_k=0$ 可以避免这个问题，但是这会使得在算法实现中没有那么简单清楚，而且，这个问题是可以得到解决的。接下来，我们对这个问题进行具体说明。\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;假设我们从参数向量 $\\theta_j$ 中减去了向量 $\\psi$，这时，每一个 $\\theta_j$ 都变成了 $\\theta_j-\\psi \\ (j=1,\\cdots,k)$。此时假设函数变成了以下的式子：\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "$$\n",
    "\\begin{align}\n",
    "p(y^{(i)}=j\\,|\\,x^{(i)};\\theta)\n",
    "&= \\frac{e^{(\\theta_j-\\psi)^Tx^{(i)}}}{\\sum^{k}_{l=1}e^{(\\theta_l-\\psi)^Tx^{(i)}}} \\\\ \n",
    "& = \\frac{e^{\\theta^T_jx^{(i)}}e^{-\\psi^Tx^{(i)}}}{\\sum^{k}_{l=1}e^{\\theta_l^Tx^{(i)}}e^{-\\psi^Tx^{(i)}}} \\\\\n",
    "&= \\frac{e^{\\theta_j^Tx^{(i)}}}{\\sum^{k}_{l=1}e^{\\theta_l^T}x^{(i)}}\n",
    "\\end{align}\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;换句话说，从 $\\theta_j$ 中减去 $\\psi$ 完全不影响假设函数的预测结果。这表明前面的softmax regression模型中存在冗余的参数。更正式一点说，softmax regression模型被过度参数化了。对于任意一个用于拟合数据的假设函数，可以求出多组参数值，这些参数得到的是完全相同的假设函数 $h_\\theta$。\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;进一步而言，如果参数 $(\\theta_1,\\theta_2,\\cdots,\\theta_k)$ 是 cost function $J(\\theta)$ 的极小值点，那么 $(\\theta_1-\\psi,\\theta_2-\\psi,\\cdots,\\theta_k-\\psi)$ 同样也是它的极小值点，其中 $\\psi$ 可以为任意向量。因此，使得 $J(\\theta)$ 最小化的解不是唯一的。（有趣的是，由于 $J(\\theta)$仍然是一个凸函数，因此梯度下降不会遇到局部最优解的问题，但是 $Hessian$ 矩阵是奇异的/不可逆的，这会直接导致采用牛顿法优化就遇到数值计算的问题）\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;注意，当 $\\phi=\\theta_k$ 时，我们总是可以将 $\\theta_k$ 替换为 $\\theta_k-\\psi=\\vec{0}$（即替换为全零向量），并且这种替换不会影响假设函数。因此，我们可以去掉参数向量 $\\theta_k$ （或者其他 $\\theta_j$ 中的任意一个）而不影响假设函数的表达能力。实际上，与其优化全部的 $k×(n+1)$ 个参数 $(\\theta_1,\\theta_2,\\cdots,\\theta_k)$ （其中，$\\theta_j\\in\\mathbb{R}^{n+1}$），我们可以令 $\\theta_k=\\vec{0}$，只优化剩余的 $(k-1)×(n+1)$ 个参数，这样算法依然能够正常工作。\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;在实际应用中，为了让算法实现更加简单清楚，往往保留所有参数 $(\\theta_1,\\theta_2,\\cdots,\\theta_k)$，而不任意地将某一参数设置为 $\\vec{0}$。但此时，我们需要对 cost function 做一个改动：加入权重衰减项。权重衰减项可以解决 softmax regression 参数冗余所带来的数值问题。\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 权重衰减"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;我们通过添加一个权重衰减项 $\\frac{\\lambda}{2}\\sum^{k}_{i=1}\\sum^{n}_{j=0}\\theta_{ij}^2$ 来修改 cost function，这个衰减项会惩罚过大的参数值，现在我们的 cost function 变为：\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "$$\n",
    "J(\\theta)=-\\frac{1}{m}\\left( \\sum^{m}_{i=1}\\sum^{k}_{j=1}1\\{y^{(i)}=j\\}\\,log\\,\\frac{e^{\\theta_j^Tx^{(i)}}}{\\sum^{k}_{l=1}e^{\\theta^T_lx^{(i)}}} \\right)+\\frac{\\lambda}{2}\\sum^{k}_{i=1}\\sum^{n}_{j=0}\\theta_{ij}^2\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;有了这个权重衰减项之后($\\lambda>0$)，Cost function 就变成了严格的凸函数，这样就可以保证得到唯一的解了。此时的 Hessian 矩阵变成可逆矩阵，并且因为 $J(\\theta)$ 是凸函数，梯度下降法和L-BFGS等算法可以保证收敛到全局最优解。\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;为了使用优化算法，我们需要求得这个新函数的 $J(\\theta)$ 的导数，如下：\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "$$\n",
    "\\triangledown_{\\theta_j}J(\\theta)=-\\frac{1}{m}\\left( x^{(i)}(1\\{y^{(i)}=j\\}-p(y^{(i)}\\,|\\,x^{(i)};\\theta)) \\right)+\\lambda\\theta_j\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;注意，这里的 $\\triangledown_{\\theta_j}J(\\theta)\\in\\mathbb{R}^{n+1}$。通过最小化 $J(\\theta)$，我们就能实现一个可用的 softmax regression 模型。\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2.4 推导 $\\frac{\\partial{J(\\theta)}}{\\partial\\theta_j}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;方法一：\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial{J(\\theta)}}{\\partial\\theta_j}\n",
    "&= -\\frac{1}{m}\\frac{\\partial}{\\partial\\theta_j}\\left[ \\sum^{m}_{i=1}\\sum^{k}_{j=1}1\\{y^{(i)}=j\\}\\,log\\,\\frac{e^{\\theta_j^Tx^{(i)}}}{\\sum^{k}_{l=1}e^{\\theta_l^Tx^{(i)}}} \\right]+\\lambda\\theta_j \\\\\n",
    "&= -\\frac{1}{m}\\frac{\\partial}{\\partial\\theta_j}\\left[ \\sum^{m}_{i=1}\\sum^{k}_{j=1}1\\{y^{(i)}=j\\}\\,(log\\,e^{\\theta_j^Tx^{(i)}}-log\\,\\sum^{k}_{l=1}e^{\\theta_l^Tx^{(i)}}) \\right]+\\lambda\\theta_j \\\\\n",
    "&= -\\frac{1}{m}\\frac{\\partial}{\\partial\\theta_j}\\left[ \\sum^{m}_{i=1}\\sum^{k}_{j=1}1\\{y^{(i)}=j\\}\\,(\\theta_j^Tx^{(i)}-log\\,\\sum^{k}_{l=1}e^{\\theta_l^Tx^{(i)}}) \\right]+\\lambda\\theta_j \\\\\n",
    "&= -\\frac{1}{m}\\frac{\\partial}{\\partial\\theta_j}\\left[ \\sum^{m}_{i=1}1\\{y^{(i)}=j\\}\\,(\\sum^{k}_{j=1}\\theta_j^Tx^{(i)}-\\sum^{k}_{j=1}log\\,\\sum^{k}_{l=1}e^{\\theta_l^Tx^{(i)}}) \\right]+\\lambda\\theta_j \\\\\n",
    "&= -\\frac{1}{m}\\left[ \\sum^{m}_{i=1}1\\{y^{(i)}=j\\}\\,(x^{(i)}-\\sum^{k}_{j=1}\\frac{x^{(i)}e^{\\theta_j^Tx^{(i)}}}{\\sum^k_{l=1}e^{\\theta_l^Tx^{(i)}}}) \\right]+\\lambda\\theta_j \\\\\n",
    "&= -\\frac{1}{m}\\left[ \\sum^{m}_{i=1}x^{(i)}\\,(1\\{y^{(i)}=j\\}-\\sum^{k}_{j=1}1\\{y^{(i)}=j\\}\\frac{e^{\\theta_j^Tx^{(i)}}}{\\sum^k_{l=1}e^{\\theta_l^Tx^{(i)}}}) \\right]+\\lambda\\theta_j \\\\\n",
    "&= -\\frac{1}{m}\\left[ \\sum^{m}_{i=1}x^{(i)}\\,(1\\{y^{(i)}=j\\}-\\frac{e^{\\theta_j^Tx^{(i)}}}{\\sum^k_{l=1}e^{\\theta_l^Tx^{(i)}}}) \\right]+\\lambda\\theta_j \\\\\n",
    "&= -\\frac{1}{m}\\left[ \\sum^{m}_{i=1}x^{(i)}\\,(1\\{y^{(i)}=j\\}-p(y^{(i)}\\,|\\,x^{(i)};\\theta)) \\right]+\\lambda\\theta_j\n",
    "\\end{align} \n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;方法二：\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial{J(\\theta)}}{\\partial\\theta_j}\n",
    "&= -\\frac{1}{m}\\left[ \\sum^{m}_{i=1}\\frac{\\partial}{\\partial{\\theta_j}}(1\\{y^{(i)}=j\\}\\,log\\,\\frac{e^{\\theta_j^Tx^{(i)}}}{\\sum^k_{l=1}e^{\\theta_l^Tx^{(i)}}}+\\sum^k_{c\\ne{j}}1\\{y^{(i)}=c\\}\\,log\\,\\frac{e^{\\theta_c^Tx^{(i)}}}{\\sum^k_{l=1}e^{\\theta_l^Tx^{(i)}}}) \\right]+\\lambda\\theta_j \\\\ \n",
    "&= -\\frac{1}{m}\\left[ \\sum^{m}_{i=1}(1\\{y^{(i)}=j\\}(x^{(i)}-\\frac{x^{(i)}e^{\\theta_j^Tx^{(i)}}}{\\sum^k_{l=1}e^{\\theta_l^Tx^{(i)}}})+\\sum^k_{c\\ne{j}}1\\{y^{(i)}=c\\}(-\\frac{x^{(i)}e^{\\theta_j^Tx^{(i)}}}{\\sum^k_{l=1}e^{\\theta_l^Tx^{(i)}}})) \\right]+\\lambda\\theta_j \\\\\n",
    "&= -\\frac{1}{m}\\left[ \\sum^{m}_{i=1}(x^{(i)}1\\{y^{(i)}=j\\}(1-\\frac{e^{\\theta_j^Tx^{(i)}}}{\\sum^k_{l=1}e^{\\theta_l^Tx^{(i)}}})+\\sum^k_{c\\ne{j}}1\\{y^{(i)}=c\\}(-\\frac{x^{(i)}e^{\\theta_j^Tx^{(i)}}}{\\sum^k_{l=1}e^{\\theta_l^Tx^{(i)}}})) \\right]+\\lambda\\theta_j \\\\\n",
    "&= -\\frac{1}{m}\\left[ \\sum^m_{i=1}x^{(i)}(1\\{y^{(i)}=j\\}-1\\{y^{(i)}=j\\}p(y^{(i)}\\,|\\,x^{(i)};\\theta))+\\sum^k_{c\\ne{j}}1\\{y^{(i)}=c\\}(-p(y^{(i)}\\,|\\,x^{(i)};\\theta)) \\right]+\\lambda\\theta_j \\\\\n",
    "&= -\\frac{1}{m}\\left[ \\sum^m_{i=1}x^{(i)}(1\\{y^{(i)}=j\\}-\\sum^k_{j=1}1\\{y^{(i)}=j\\}p(y^{(i)}\\,|\\,x^{(i)};\\theta) \\right]+\\lambda\\theta_j \\\\\n",
    "&= -\\frac{1}{m}\\left[ \\sum^{m}_{i=1}x^{(i)}\\,(1\\{y^{(i)}=j\\}-p(y^{(i)}\\,|\\,x^{(i)};\\theta)) \\right]+\\lambda\\theta_j\n",
    "\\end{align}\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2.5 矩阵化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;因为 $y\\in\\{1,2,\\cdots,k\\}$，所以，对 $y$ 进行独热编码。即 $y^{(i)}\\in\\mathbb{R}^{k}$，其中，若 $y^{(i)}$ 属于类别 $i$，则 $y^{(i)}$ 第 $i$ 个位置上的元素为1，其余位置元素为0。因此，定义矩阵 $G$ 为\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "$$\n",
    "G=\n",
    "\\begin{bmatrix}\n",
    "-(y^{(1)})^T- \\\\ -(y^{(2)})^T- \\\\ \\vdots \\\\ -(y^{(m)})^T-\n",
    "\\end{bmatrix}_{m×k}\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;定义概率矩阵 $P$ 为\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "$$\n",
    "P_{m×k} = norm(\\,exp(X_{m×(n+1)}\\cdot\\theta_{(n+1)×k}^T)\\,)\\quad\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;其中，norm 表示归一化项，因此，概率矩阵 $P$ 的具体计算方式为：首先，计算 $exp(X\\theta^T)$ 得到 $m×k$ 的矩阵。其次，使用 $np.sum()$ 对该该矩阵按行进行求和，得到 m×1 的矩阵。最后，利用Python的广播（broadcast）机制，将该矩阵与 $exp(X\\theta^T)$ 对应位置元素进行相乘（element-wise multiplication）得到概率矩阵 $P$。\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;于是\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "$$\n",
    "\\frac{\\partial{J(\\theta)}}{\\partial\\theta}\n",
    "=-\\frac{1}{m}(G-P)^T\\cdot{X}+\\lambda\\theta\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;所以，cost function为\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "$$\n",
    "J(\\theta)=-np.mean(G\\circ{P})+\\lambda \\ {np.sum(\\theta)}\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;其中，$\\circ$ 表示对应位置元素相乘，即 element-wise multiplication。\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3. 实现 softmax regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.1 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>traget</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                5.1               3.5                1.4               0.2   \n",
       "1                4.9               3.0                1.4               0.2   \n",
       "2                4.7               3.2                1.3               0.2   \n",
       "3                4.6               3.1                1.5               0.2   \n",
       "4                5.0               3.6                1.4               0.2   \n",
       "\n",
       "   traget  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "features = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "label = pd.DataFrame(data=iris.target, columns=['traget'])\n",
    "data = pd.concat([features, label], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadData(df):\n",
    "    ones = pd.DataFrame({'ones': np.ones(len(df))})\n",
    "    df = pd.concat([ones, df], axis=1)\n",
    "    X = df.iloc[:,:-1].values\n",
    "    y = df.iloc[:,-1].values\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150, 5), (150,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = loadData(data)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.2 数据可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAs4AAAHvCAYAAABE9FkiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIiIiIipI4RwRERERUcH/ALdLqYpCV7y+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc8f96c7940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(X[:, 3][y==0], X[:, 4][y==0], marker='x', label=iris.target_names[0])\n",
    "plt.scatter(X[:, 3][y==1], X[:, 4][y==1], marker='^', label=iris.target_names[1])\n",
    "plt.scatter(X[:, 3][y==2], X[:, 4][y==2], marker='+', label=iris.target_names[2])\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel(iris.feature_names[2])\n",
    "plt.ylabel(iris.feature_names[3])\n",
    "plt.title('Iris Data Set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Softmax Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;对 $y$ 进行独热编码，得到矩阵G。\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def oneHotY(y):\n",
    "    # m为样本数\n",
    "    m = y.shape[0]\n",
    "    # k为类别数\n",
    "    k = len(np.unique(y))\n",
    "    \n",
    "    oneHotY = np.zeros((m, k))\n",
    "    for i in range(k):\n",
    "        oneHotY[:, i] = (y==i)\n",
    "        \n",
    "    return oneHotY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G = oneHotY(y)\n",
    "G.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initializeWithZeros(X, y):\n",
    "    k = len(np.unique(y))\n",
    "    \n",
    "    return np.zeros((k, X.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def probabilityMatrix(X, theta):\n",
    "    expScore = np.exp(X @ theta.T)\n",
    "    sumScore = np.sum(expScore, axis=1).reshape(-1, 1)\n",
    "    \n",
    "    return np.multiply(expScore, 1 / sumScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeCost(X, G, theta, l):\n",
    "    P = probabilityMatrix(X, theta)\n",
    "    return -np.mean(np.multiply(G, np.log(P))) + l * theta.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeGradient(X, G, theta, l):\n",
    "    m = X.shape[0]\n",
    "    P = probabilityMatrix(X, theta)\n",
    "    grad = -((G-P).T @ X) / m + l * theta\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def batchGradientDescent(X, G, theta, alpha, iters, l, printFlag=True):\n",
    "    costs = np.zeros(iters)\n",
    "    \n",
    "    for i in range(iters):\n",
    "        theta = theta - alpha * computeGradient(X, G, theta, l)\n",
    "        costs[i] = computeCost(X, G, theta, l)\n",
    "        \n",
    "        if printFlag and i % 1000 == 0:\n",
    "            print(costs[i])\n",
    "        \n",
    "    return theta, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(X, theta):\n",
    "    P = probabilityMatrix(X, theta)\n",
    "    \n",
    "    return np.argmax(P, axis=1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36204899345585234\n",
      "0.15207044998459318\n",
      "0.14597815071859724\n",
      "0.14507033494591312\n",
      "0.14491252983436856\n",
      "0.14488189983190422\n",
      "0.14487517819681003\n",
      "0.14487351365380918\n",
      "0.14487305861140318\n",
      "0.144872925162167\n"
     ]
    }
   ],
   "source": [
    "X, y = loadData(data)\n",
    "G = oneHotY(y)\n",
    "theta = initializeWithZeros(X, y)\n",
    "\n",
    "iters = 10000\n",
    "alpha = 0.01\n",
    "l = 0.1\n",
    "\n",
    "theta, costs = batchGradientDescent(X, G, theta, alpha, iters, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtQAAAHvCAYAAAB0eEgDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ntb93DQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc8f96adcf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(np.arange(iters), costs, color='red')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Error vs. Training Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        50\n",
      "           1       1.00      0.88      0.94        50\n",
      "           2       0.89      1.00      0.94        50\n",
      "\n",
      "    accuracy                           0.96       150\n",
      "   macro avg       0.96      0.96      0.96       150\n",
      "weighted avg       0.96      0.96      0.96       150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict(X, theta)\n",
    "print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4. Softmax Regression 与 Logistic Regression 的关系"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;当类别数 $k=2$ 时，softmax regression 退化为 logistic regression。这表明 softmax regression 是 logistic regression 的一般形式。具体地说，当 $k=2$ 时，softmax regression 的假设函数为：\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "$$\n",
    "h_\\theta(x)=\\frac{1}{e^{\\theta_1^Tx}+e^{\\theta_2^T}x}\n",
    "\\begin{bmatrix}\n",
    "e^{\\theta_1^Tx} \\\\\n",
    "e^{\\theta_2^Tx}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;利用 softmax regression 回归参数冗余的特点，我们令 $\\psi=\\theta_1$，并且从两个参数向量中都减去向量 $\\theta_1$，得到：\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "$$\n",
    "\\begin{align}\n",
    "h(x) &= \\frac{1}{e^{\\vec{0}^Tx}+e^{(\\theta_2-\\theta_1)^Tx}}\n",
    "\\begin{bmatrix}\n",
    "e^{\\vec{0}^Tx} \\\\\n",
    "e^{(\\theta_1-\\theta_2)^Tx}\n",
    "\\end{bmatrix} \\\\\n",
    "&= \n",
    "\\begin{bmatrix}\n",
    "\\frac{1}{1+e^{(\\theta_1-\\theta_2)^Tx}} \\\\\n",
    "\\frac{e^{(\\theta_1-\\theta_2)^Tx}}{1+e^{(\\theta_1-\\theta_2)^Tx}}\n",
    "\\end{bmatrix} \\\\\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "\\frac{1}{1+e^{(\\theta_1-\\theta_2)^Tx}} \\\\\n",
    "1-\\frac{1}{1+e^{(\\theta_1-\\theta_2)^Tx}}\n",
    "\\end{bmatrix} \n",
    "\\end{align}\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;因此，用 $\\theta'$ 来表示 $\\theta_1-\\theta_2$，我们就会发现 softmax regression 预测其中一个类别的概率为 $\\frac{1}{1+e^{(\\theta')^Tx}}$，另一个类别的概率为 $1-\\frac{1}{1+e^{(\\theta')^Tx}}$，这与logistic regression 是一致的。\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Softmax Regression vs. k 个二元分类器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;如果你在开发一个音乐分类的应用，需要对 $k$ 种类型的音乐进行识别，那么是选择使用 softmax regression，还是使用 logistic regression 建立 $k$ 个独立的二元分类器呢？\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;这一选择取决于你的类别之间是否互斥，例如，如果你有四个类别的音乐，分别为：古典音乐、乡村音乐、摇滚乐和爵士乐，那么你可以假设每个训练样本只会被打上一个标签（即一首歌只能属于这四种音乐类型的其中一种），此时，你应该使用类别数 $k=4$ 的 softmax regression（如果在你的数据集中，有的歌曲不属于以上四类的其中任何一类，那么你可以添加一个”其他类“，并将类别数 $k$ 设为5）。\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;如果你的四个类别如下：人声音乐、舞曲、影视原声、流行歌曲，那么这些类别之间并不是互斥的。例如：一首歌曲可以来源于影视原声，同时也包含人声。在这种情况下，使用4个二分类的 logistic regression 更为合适。这样每个新的音乐作品，我们的算法可以分别判断它是否属于各个类别。\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;现在，我们来看一个计算机视觉领域的例子，你的任务是将图像分到三个不同的类别中。（1）假设这三个类别分别是：室内场景、户外城区场景、户外荒野场景。你会使用softmax regression 还是3个 logistic regression 呢？（2）现在假设这三个类别分别是室内场景、黑白图片、包含人物的图片，你会选择 softmax regression 还是多个 logistic regression 呢？\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;在第一个例子中，三个类别是互斥的，因此，更适于选择 softmax regression。而在第二个例子中，建立三个独立的 logistic regression 更加合适。\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
