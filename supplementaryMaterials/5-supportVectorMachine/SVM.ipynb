{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在讨论 SVM 的时候，出于简化的目的，我们需要引进新的符号来表示分类。假设我们要对一个二分类问题建立一个线性分类器，其中，标签(label)为 $y$，特征(feature)为 $x$。从现在开始，我们使用 $y\\in\\{-1,1\\}$ 来表示类别 (而不是之前的 $y\\in\\{0,1\\}$)。同时，我们也不再使用向量 $\\theta$ 来表示线性分类器的参数，而是使用参数 $w$ 和 $b$，所以，我们的线性分类器为"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "h_{w,b}(x)=g(w^Tx+b)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中，当 $z\\geq0$ 时，$g(z)=1$；当 $z<0$ 时，$g(z)=-1$。这里的参数 \"$w,b$\" 可以让我们把截距项(intercept term) $b$ 和其他参数分开。（此外，我们不需要像之前那样设定 $x_0=1$。）因此，这里的参数 $b$ 就相当于之前的 $\\theta_0$，而参数 $w$ 则相当于 $[\\theta_1,\\cdots,\\theta_n]^T$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要注意的是，根据我们对函数 $g$ 的定义，我们的分类器会直接预测类别是1或者-1（参考感知机算法 perceptron algorithm ），这样也就不需要经过中间步骤来估计 $y$ 为1的概率。（这里的中间步骤指的是逻辑回归的步骤）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Functional and geometric margins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给定一个训练样本 $(x^{(i)},y^{(i)})$，我们定义该训练样本的函数间隔(functional margin)为"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{\\gamma}^{(i)}=y^{(i)}(w^Tx^{(i)}+b)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意到，为了使函数间隔尽可能大（也就是说，为了让我们的预测是正确的且有着高的确信度），当 $y^{(i)}=1$ 时，我们需要尽可能地使 $w^Tx^{(i)}+b$ 是一个大的正数。相反，当 $y^{(i)}=-1$ 时，我们需要让 $w^Tx^{(i)}+b$ 尽可能是一个大的负数。而且，如果 $y^{(i)}(w^Tx+b)>0$，则说明我们对这个训练样本的预测是正确的。因此，一个大的函数间隔代表着一个正确且有着高确信度的预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "给定一个训练集 $S=\\{(x^{(i)},y^{(i)}),i=1,\\cdots,m\\}$，我们定义该训练集的函数间隔为所有训练样本的函数间隔的最小值。记为 $\\hat{\\gamma}$，即"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$$\n",
    "\\hat{\\gamma}=\\min_{i=1,\\cdots,m}\\hat{\\gamma}^{(i)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于一个线性分类器，选择上面给定的函数 $g$ (取值范围为 $\\{-1,1\\}$)。我们注意到函数间隔有一个性质，使得它不能够很好地衡量确信度。如果我们将 $w$ 和 $b$ 换成 $2w$ 和 $2b$，那么由于 $g(w^Tx+b)=g(2w^Tx+2b)$，这将不会改变 $h_{w,b}(x)$。也就是说，函数 $g$ 和 $h_{w,b}(x)$ 只取决于 $w^Tx+b$ 的正负符号(sign)，但不受其数值大小(magnitude)的影响。但是，把 $(w,b)$ 替换成 $(2w,2b)$ 会导致函数间隔被放大了两倍。换句话说，成比例地缩放 $w$ 和 $b$ 后，超平面(hyperplane)并没有变化，而函数间隔却变化了。直观地看，这导致我们需要引入某种归一化条件，例如，$\\Vert{w}\\Vert=1$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "因此，给定一个训练样本 $(x^{(i)},y^{(i)})$ 我们定义该训练样本的几何间隔(geometric margins)为"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$$\n",
    "\\gamma^{(i)}=y^{(i)}\\left( \\left(\\frac{w}{\\Vert{w}\\Vert} \\right)^Tx^{(i)}+\\frac{b}{\\Vert{w}\\Vert} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "这样，当我们成比例地缩放 $w$ 和 $b$ 的时候，几何间隔的大小是不变的。值得注意的是，正是这种对参数缩放的不变性，当我们试图对某个训练集拟合 $w$ 和 $b$ 的时候，我们可以对 $w$ 添加任意的缩放约束。例如，我们可以要求 $\\Vert{w}\\Vert=1，\\left|w_1\\right|=5，\\left|w_1+b\\right|+\\left|w_2\\right|=2$ 等等，这些只要对 $w$ 和 $b$ 进行成比例的缩放就可以满足。而且由于是成比例地缩放，所以并不改变几何间隔的大小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后，给定一个训练集 $S=\\{(x^{(i)},y^{(i)}),i=1,\\cdots,m\\}$，我们也可以定义该训练集的几何间隔为所有训练样本的几何间隔的最小值。记为 $\\gamma$，即"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\gamma=\\min_{i=1,\\cdots,m}\\gamma^{(i)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，根据函数间隔和几何间隔的定义可知，函数间隔和几何间隔有如下关系："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\gamma^{(i)} &= \\frac{\\hat\\gamma^{(i)}}{\\Vert{w}\\Vert} \\\\\n",
    "\\gamma &= \\frac{\\hat\\gamma}{\\Vert{w}\\Vert} \n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**小结**："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们的目的是要建立一个最大间隔分类器。具体地说，我们希望最大化整个训练集的间隔，而整个训练集的间隔被定义为所有训练样本间隔的最小值。而之所以不采用函数间隔来衡量训练样本到超平面的距离，是因为对于任意一个成功划分训练样本的超平面来说，我们只要成比例地放大 $w$ 和 $b$ 就可以让函数间隔任意大。这样我们就无法找到最优的超平面。因此，我们使用几何间隔来衡量训练样本到超平面的距离。而几何间隔其实就是高中学的点到直线距离的推广。而由于几何间隔对于参数缩放的不变性，使得我们下面可以使用这个性质简化我们的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The optimal margin classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "到目前为止，我们假设给定的训练集是线性可分的(linearly separable)。也就是说，能够在正负训练样本之间用某种分离超平面(separating hyperplane)进行划分。我们的希望是能够找到使得几何间隔最大的分离超平面。因此，我们可以提出如下的优化问题："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "max_{\\gamma,w,b}&\\quad\\gamma\\\\\n",
    "s.t.&\\quad y^{(i)}\\left( \\left(\\frac{w}{\\Vert{w}\\Vert}\\right)^Tx^{(i)}+\\frac{b}{\\Vert{w}\\Vert} \\right)\\geq\\gamma,\\quad i=1,\\cdots,m\\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "即我们希望最大化超平面关于训练集的几何间隔 $\\gamma$，约束条件表示的是超平面与每个训练样本之间的几何间隔至少是 $\\gamma$。因为几何间隔和函数间隔可以通过 $\\gamma=\\frac{\\hat\\gamma}{\\Vert{w}\\Vert}$ 联系起来，所以可以将问题改写为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "max_{\\gamma,w,b}&\\quad \\frac{\\hat\\gamma}{\\Vert{w}\\Vert}\\\\\n",
    "s.t.&\\quad y^{(i)}(w^Tx^{(i)}+b)\\geq\\hat\\gamma,\\quad i=1,\\cdots,m\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "但注意到，此时我们的目标函数 $\\frac{\\hat\\gamma}{\\Vert{w}\\Vert}$ 是非凸的(non-convex)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上一节，我们提到参数的成比例缩放不改变几何间隔的大小，这使得我们可以对 $w$ 和 $b$ 的缩放增加约束。基于这个性质，我们可以令函数间隔 $\\hat\\gamma=1$。也就是说，我们对 $w$ 和 $b$ 的成比例缩放要使得函数间隔为1。因此"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "max_{\\gamma,w,b}&\\quad \\frac{1}{\\Vert{w}\\Vert}\\\\\n",
    "s.t.&\\quad y^{(i)}(w^Tx^{(i)}+b)\\geq1,\\quad i=1,\\cdots,m\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于最大化 $\\frac{1}{\\Vert{w}\\Vert}$ 和最小化 $\\frac{1}{2}\\Vert{w}\\Vert^2$ 等价，因此，可得"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "min_{\\gamma,w,b}&\\quad \\frac{1}{2}\\Vert{w}\\Vert^2\\\\\n",
    "s.t.&\\quad y^{(i)}(w^Tx^{(i)}+b)\\geq1,\\quad i=1,\\cdots,m\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过这样的转化，这个问题就比较容易解决了。上面的问题是一个凸二次规划(convex quadratic programming)问题。此时，我们可以调用成熟的商业QP软件包对此进行求解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们先岔开话题，介绍一下拉格朗日对偶(Lagrange duality)，这样会引出我们这个优化问题的对偶形式。通过这种对偶形式，我们可以推出一种非常有效的算法，来解决上面这个优化问题，而且通常这个算法比通用的QP软件更好用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Lagrange duality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于任意一个带约束的优化问题，我们可以将其写成这样的形式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "min&\\quad f_0(x)\\\\\n",
    "s.t.&\\quad f_i(x)\\leq0,\\quad i=1,\\cdots,m\\\\\n",
    "&\\quad h_i(x)=0,\\quad i=1,\\cdots,p\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假如 $f_0,f_1,\\cdots,f_m$ 全都是凸函数，并且 $h_1,\\cdots,h_p$ 全都是仿射函数，那么这个问题就叫做凸优化(convex optimization)问题。凸优化问题有许多优良的性质。不过，这里我们并没有假定我们要处理的问题是凸优化问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
