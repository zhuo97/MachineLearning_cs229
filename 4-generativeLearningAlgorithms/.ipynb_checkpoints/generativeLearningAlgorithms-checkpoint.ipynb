{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;So far, we've mainly been talking about learning algorithms that model $p(y\\,|\\,x;\\theta)$, the conditional distribution of $y$ given $x$. For instance, logistic regression modeled $p(y\\,|\\,x;\\theta)$ as $h_\\theta(x)=g(\\theta^Tx)$ where $g$ is the sigmoid function. In these notes, we'll talk about a different type of learning algorithm.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;Consider a classification problem in which we want to learn to distinguish between elephants $(y=1)$ and dogs $(y=0)$, based on some features of an animal. Given a training set, an algorithm like logistic regression or the perceptron algorithm (basically) tries to find a straight line——that is, a decision boundary——that separates the elephants and dogs. Then, to classify a new animal as either an elephant or a dog, it checks on which side of the decision boundary it falls, and makes its prediction accordingly.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;Here's a different approach. First, looking at elephants, we can build a model of what elephants look like. Then, looking at dogs, we can build a separate model of what dogs look like. Finally, to classify a new animal, we can match the new animal against the elephant model, and match it against the dog model, to see whether the new animal looks more like the elephants or more like the dogs we had seen in the training set.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;Algorithm that try to learn $p(y\\,|\\,x;\\theta)$ directly (such as logistic regression), or algorithms that try to learn mappings directly from the space of inputs $\\mathcal{X}$ to the labels $\\{0,1\\}$, (such as the perceptron algorithm) are called **discriminative learning algorithms**.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;Here, we'll talk about algorithms that instead try to model $p(x\\,|\\,y)$ (and $p(y)$). These algorithms are called **generative learning algorithms**. For instance, if $y$ indicates whether a example is a dog (0) or an elephant (1), then $p(x\\,|\\,y=0)$ models the distribution of dog's features, and $p(x\\,|\\,y=1)$ models the distribution of elephants' fearures.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;After modeling $p(y)$ (called the **class priors**) and $p(x\\,|\\,y)$, our algorithm can then use Bayes rule to derive the posterior distribution on $y$ given $x$:\n",
    "$$\n",
    "p(y\\,|\\,x)=\\frac{p(x\\,|\\,y)p(y)}{p(x)}\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "&emsp;&emsp;Here, the denominator is given by $p(x)=p(x\\,|\\,y=1)p(y=1)+p(x\\,|\\,y=0)p(y=0)$, and thus can also be expressed in terms of the quantities $p(x\\,|\\,y)$ and $p(y)$ that we've learned. Actually, if we're calculating $p(y\\,|\\,x)$ in order to make a prediction, then we don't actually need to calculate the denominator, since\n",
    "$$\n",
    "\\begin{align}\n",
    "arg\\max_{y}p(y\\,|\\,x)\n",
    "&= arg\\max_{y}\\frac{p(x\\,|\\,y)p(y)}{p(x)} \\\\\n",
    "&= arg\\max_{y}p(x\\,|\\,y)\\,p(y)\n",
    "\\end{align}\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
